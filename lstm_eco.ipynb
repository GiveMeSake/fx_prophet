{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1859,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import pandas_ta as ta\n",
    "import tpqoa\n",
    "import sys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1860,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Params\n",
    "day_ahead = 1\n",
    "epochs = 100\n",
    "lstm_units = 100\n",
    "batch_size = 32\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1861,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./data/EUR_USD_D_2006_2024_M.csv', parse_dates = ['time'], usecols = ['time', 'c', 'h', 'l'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge macroeco data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1862,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_eco = data.copy()\n",
    "data_eco.drop(['h', 'l'], axis=1, inplace=True)\n",
    "data_eu_interest = pd.read_csv('./data/Interest_rate_EUR.csv')\n",
    "data_ge_interest = pd.read_csv('./data/Interest_rate_Germany.csv')\n",
    "data_sp500 = pd.read_csv('./data/SP500_Historical_Data.csv', parse_dates = ['Date'])\n",
    "data_cpi = pd.read_csv('./data/CPI-U_Inflation_US.csv')\n",
    "data_dff = pd.read_csv('./data/DFF.csv')\n",
    "data_gdaxi = pd.read_csv('./data/GDAXI.csv')\n",
    "data_hcip = pd.read_csv('./data/HCIP_Inflation_EUR.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1863,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_eco['year_month'] = data_eco['time'].dt.strftime('%Y-%m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1864,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge with data_eu_interest\n",
    "data_eu_interest['DATE'] = pd.to_datetime(data_eu_interest['DATE'], format=\"%Y-%m-%d\").dt.strftime(\"%Y-%m\")\n",
    "data_eco = pd.merge(data_eco, data_eu_interest, left_on='year_month', right_on='DATE', how='left')\n",
    "data_eco.drop(['DATE', 'TIME PERIOD'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1865,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge with data_ge_interest\n",
    "data_ge_interest['DATE'] = pd.to_datetime(data_ge_interest['DATE'], format=\"%Y-%m-%d\").dt.strftime(\"%Y-%m\")\n",
    "data_eco = pd.merge(data_eco, data_ge_interest, left_on='year_month', right_on='DATE', how='left')\n",
    "data_eco.drop(['DATE', 'TIME PERIOD'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1866,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge with data_sp500\n",
    "data_eco = pd.merge(data_eco, data_sp500[['Date', 'Adj Close']], left_on='time', right_on='Date', how='left')\n",
    "data_eco.drop(['Date'], axis=1, inplace=True)\n",
    "data_eco['Adj Close'].interpolate(method='linear', inplace=True)\n",
    "data_eco['Adj Close'].bfill(inplace=True)\n",
    "data_eco.rename(columns= {'Adj Close' : 'sp500'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1867,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge with data_cpi\n",
    "data_cpi = data_cpi[~data_cpi['Period'].isin(['S01', 'S02'])]\n",
    "data_cpi['year_month'] = data_cpi['Year'].astype(str) + '-' + pd.to_datetime(data_cpi['Period'], format=\"M%m\").dt.strftime('%m')\n",
    "data_eco = pd.merge(data_eco, data_cpi[['year_month', 'Value']], left_on='year_month', right_on='year_month', how='left')\n",
    "data_eco.rename(columns= {'Value' : 'cpi-u'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1868,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge with data_dff\n",
    "data_dff['DATE'] = pd.to_datetime(data_dff['DATE'], format='%Y-%m-%d')\n",
    "data_eco = pd.merge(data_eco, data_dff[['DATE', 'DFF']], left_on='time', right_on='DATE', how='left')\n",
    "data_eco.rename(columns= {'Value' : 'cpi-u'}, inplace=True)\n",
    "data_eco.drop(['DATE'], axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1869,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge with data_gdaxi\n",
    "data_gdaxi['Date'] = pd.to_datetime(data_gdaxi['Date'], format='%Y-%m-%d')\n",
    "data_eco = pd.merge(data_eco, data_gdaxi[['Date', 'Adj Close']], left_on='time', right_on='Date', how='left')\n",
    "data_eco.rename(columns= {'Adj Close' : 'gdaxi'}, inplace=True)\n",
    "data_eco.drop(['Date'], axis=1, inplace=True)\n",
    "data_eco['gdaxi'].interpolate(method='linear', inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1870,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>c</th>\n",
       "      <th>year_month</th>\n",
       "      <th>interest_rate_eu</th>\n",
       "      <th>interest_rate_ge</th>\n",
       "      <th>sp500</th>\n",
       "      <th>cpi-u</th>\n",
       "      <th>DFF</th>\n",
       "      <th>gdaxi</th>\n",
       "      <th>hcip</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2006-01-02</td>\n",
       "      <td>1.18380</td>\n",
       "      <td>2006-01</td>\n",
       "      <td>3.38</td>\n",
       "      <td>3.32</td>\n",
       "      <td>1268.800049</td>\n",
       "      <td>2.1</td>\n",
       "      <td>4.09</td>\n",
       "      <td>5449.979980</td>\n",
       "      <td>2.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2006-01-03</td>\n",
       "      <td>1.18420</td>\n",
       "      <td>2006-01</td>\n",
       "      <td>3.38</td>\n",
       "      <td>3.32</td>\n",
       "      <td>1268.800049</td>\n",
       "      <td>2.1</td>\n",
       "      <td>4.34</td>\n",
       "      <td>5460.680176</td>\n",
       "      <td>2.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2006-01-04</td>\n",
       "      <td>1.20310</td>\n",
       "      <td>2006-01</td>\n",
       "      <td>3.38</td>\n",
       "      <td>3.32</td>\n",
       "      <td>1273.459961</td>\n",
       "      <td>2.1</td>\n",
       "      <td>4.22</td>\n",
       "      <td>5523.620117</td>\n",
       "      <td>2.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2006-01-05</td>\n",
       "      <td>1.21140</td>\n",
       "      <td>2006-01</td>\n",
       "      <td>3.38</td>\n",
       "      <td>3.32</td>\n",
       "      <td>1273.479980</td>\n",
       "      <td>2.1</td>\n",
       "      <td>4.24</td>\n",
       "      <td>5516.529785</td>\n",
       "      <td>2.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2006-01-06</td>\n",
       "      <td>1.21140</td>\n",
       "      <td>2006-01</td>\n",
       "      <td>3.38</td>\n",
       "      <td>3.32</td>\n",
       "      <td>1285.449951</td>\n",
       "      <td>2.1</td>\n",
       "      <td>4.22</td>\n",
       "      <td>5536.319824</td>\n",
       "      <td>2.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4733</th>\n",
       "      <td>2024-03-25</td>\n",
       "      <td>1.08030</td>\n",
       "      <td>2024-03</td>\n",
       "      <td>2.90</td>\n",
       "      <td>2.35</td>\n",
       "      <td>5218.189941</td>\n",
       "      <td>3.8</td>\n",
       "      <td>5.33</td>\n",
       "      <td>18261.310547</td>\n",
       "      <td>2.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4734</th>\n",
       "      <td>2024-03-26</td>\n",
       "      <td>1.08385</td>\n",
       "      <td>2024-03</td>\n",
       "      <td>2.90</td>\n",
       "      <td>2.35</td>\n",
       "      <td>5203.580078</td>\n",
       "      <td>3.8</td>\n",
       "      <td>5.33</td>\n",
       "      <td>18384.349609</td>\n",
       "      <td>2.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4735</th>\n",
       "      <td>2024-03-27</td>\n",
       "      <td>1.08302</td>\n",
       "      <td>2024-03</td>\n",
       "      <td>2.90</td>\n",
       "      <td>2.35</td>\n",
       "      <td>5248.490234</td>\n",
       "      <td>3.8</td>\n",
       "      <td>5.33</td>\n",
       "      <td>18477.089844</td>\n",
       "      <td>2.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4736</th>\n",
       "      <td>2024-03-28</td>\n",
       "      <td>1.08138</td>\n",
       "      <td>2024-03</td>\n",
       "      <td>2.90</td>\n",
       "      <td>2.35</td>\n",
       "      <td>5254.350098</td>\n",
       "      <td>3.8</td>\n",
       "      <td>5.33</td>\n",
       "      <td>18492.490234</td>\n",
       "      <td>2.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4737</th>\n",
       "      <td>2024-03-29</td>\n",
       "      <td>1.07866</td>\n",
       "      <td>2024-03</td>\n",
       "      <td>2.90</td>\n",
       "      <td>2.35</td>\n",
       "      <td>5254.350098</td>\n",
       "      <td>3.8</td>\n",
       "      <td>5.33</td>\n",
       "      <td>18492.490234</td>\n",
       "      <td>2.4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4738 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           time        c year_month  interest_rate_eu  interest_rate_ge  \\\n",
       "0    2006-01-02  1.18380    2006-01              3.38              3.32   \n",
       "1    2006-01-03  1.18420    2006-01              3.38              3.32   \n",
       "2    2006-01-04  1.20310    2006-01              3.38              3.32   \n",
       "3    2006-01-05  1.21140    2006-01              3.38              3.32   \n",
       "4    2006-01-06  1.21140    2006-01              3.38              3.32   \n",
       "...         ...      ...        ...               ...               ...   \n",
       "4733 2024-03-25  1.08030    2024-03              2.90              2.35   \n",
       "4734 2024-03-26  1.08385    2024-03              2.90              2.35   \n",
       "4735 2024-03-27  1.08302    2024-03              2.90              2.35   \n",
       "4736 2024-03-28  1.08138    2024-03              2.90              2.35   \n",
       "4737 2024-03-29  1.07866    2024-03              2.90              2.35   \n",
       "\n",
       "            sp500  cpi-u   DFF         gdaxi  hcip  \n",
       "0     1268.800049    2.1  4.09   5449.979980   2.4  \n",
       "1     1268.800049    2.1  4.34   5460.680176   2.4  \n",
       "2     1273.459961    2.1  4.22   5523.620117   2.4  \n",
       "3     1273.479980    2.1  4.24   5516.529785   2.4  \n",
       "4     1285.449951    2.1  4.22   5536.319824   2.4  \n",
       "...           ...    ...   ...           ...   ...  \n",
       "4733  5218.189941    3.8  5.33  18261.310547   2.4  \n",
       "4734  5203.580078    3.8  5.33  18384.349609   2.4  \n",
       "4735  5248.490234    3.8  5.33  18477.089844   2.4  \n",
       "4736  5254.350098    3.8  5.33  18492.490234   2.4  \n",
       "4737  5254.350098    3.8  5.33  18492.490234   2.4  \n",
       "\n",
       "[4738 rows x 10 columns]"
      ]
     },
     "execution_count": 1870,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merge with data_hcip\n",
    "data_hcip['DATE'] = pd.to_datetime(data_hcip['DATE'], format=\"%Y-%m-%d\").dt.strftime(\"%Y-%m\")\n",
    "data_eco = pd.merge(data_eco, data_hcip, left_on='year_month', right_on='DATE', how='left')\n",
    "data_eco.drop(['DATE', 'TIME PERIOD'], axis=1, inplace=True)\n",
    "data_eco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1871,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4712, 10)\n",
      "           time        c year_month  interest_rate_eu  interest_rate_ge  \\\n",
      "26   2006-02-07  1.19646    2006-02              3.53              3.47   \n",
      "27   2006-02-08  1.19816    2006-02              3.53              3.47   \n",
      "28   2006-02-09  1.19626    2006-02              3.53              3.47   \n",
      "29   2006-02-10  1.19840    2006-02              3.53              3.47   \n",
      "30   2006-02-13  1.18960    2006-02              3.53              3.47   \n",
      "...         ...      ...        ...               ...               ...   \n",
      "4733 2024-03-25  1.08030    2024-03              2.90              2.35   \n",
      "4734 2024-03-26  1.08385    2024-03              2.90              2.35   \n",
      "4735 2024-03-27  1.08302    2024-03              2.90              2.35   \n",
      "4736 2024-03-28  1.08138    2024-03              2.90              2.35   \n",
      "4737 2024-03-29  1.07866    2024-03              2.90              2.35   \n",
      "\n",
      "            sp500  cpi-u   DFF         gdaxi  hcip  \n",
      "26    1254.780029    2.1  4.47   5672.919922   2.4  \n",
      "27    1265.650024    2.1  4.48   5666.410156   2.4  \n",
      "28    1263.780029    2.1  4.52   5743.680176   2.4  \n",
      "29    1266.989990    2.1  4.51   5701.470215   2.4  \n",
      "30    1262.859985    2.1  4.44   5756.330078   2.4  \n",
      "...           ...    ...   ...           ...   ...  \n",
      "4733  5218.189941    3.8  5.33  18261.310547   2.4  \n",
      "4734  5203.580078    3.8  5.33  18384.349609   2.4  \n",
      "4735  5248.490234    3.8  5.33  18477.089844   2.4  \n",
      "4736  5254.350098    3.8  5.33  18492.490234   2.4  \n",
      "4737  5254.350098    3.8  5.33  18492.490234   2.4  \n",
      "\n",
      "[4712 rows x 10 columns]\n",
      "time                0\n",
      "c                   0\n",
      "year_month          0\n",
      "interest_rate_eu    0\n",
      "interest_rate_ge    0\n",
      "sp500               0\n",
      "cpi-u               0\n",
      "DFF                 0\n",
      "gdaxi               0\n",
      "hcip                0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Drop the first 26 rows to match with data_tech\n",
    "data_eco = data_eco.iloc[26:]\n",
    "print(data_eco.shape)\n",
    "print(data_eco)\n",
    "print(data_eco.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determine threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1872,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkQAAAHHCAYAAABeLEexAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8g+/7EAAAACXBIWXMAAA9hAAAPYQGoP6dpAABAo0lEQVR4nO3deVhV5f7+8XszigPgCKI4lHMaGqaSUyYFaaVmk/NAaaZZznlOjg045JRZ1qnUOpZDedSv5oBoWWqa5myOaeYAjoCYArKf3x9e7F9bUAGZbL1f17UvXWs9a+3Phw1yu9az9rYZY4wAAAAszCW/CwAAAMhvBCIAAGB5BCIAAGB5BCIAAGB5BCIAAGB5BCIAAGB5BCIAAGB5BCIAAGB5BCIAAGB5BCIgj1WqVEndu3fP7zL+8SZOnKh77rlHrq6uqlu3bq4+V0F/Tb///nvZbDZ98803+V0KUGARiIA7MHv2bNlsNm3dujXD7Q8//LBq1659x8/z3XffafTo0Xd8HKtYvXq1hg4dqsaNG2vWrFl69913bzq2e/fustlsjoe3t7eCgoI0adIkJSUl5WHV2fP999/r6aeflr+/vzw8PFSmTBk9+eSTWrRoUX6XBtxV3PK7AMBqDhw4IBeXrP1f5LvvvtOMGTMIRZm0du1aubi46LPPPpOHh8dtx3t6eurTTz+VJMXFxenbb7/V4MGD9csvv2jevHm33T87r2lOGDVqlMaOHauqVauqd+/eqlixos6fP6/vvvtO7du319y5c9WxY8c8rwu4GxGIgDzm6emZ3yVk2eXLl1WkSJH8LiPTzpw5Iy8vr0yFIUlyc3NT586dHcuvvPKKGjZsqPnz52vy5MkKCAhIt48xRlevXpWXl1e+vKbffPONxo4dq2eeeUZfffWV3N3dHduGDBmiVatWKSUlJc/rAu5WXDID8tiN801SUlI0ZswYVa1aVYUKFVLJkiXVpEkTRUVFSbp+SWfGjBmS5HRpJ83ly5c1aNAgBQYGytPTU9WrV9d7770nY4zT8165ckX9+/dXqVKlVKxYMT311FM6efKkbDab05mn0aNHy2azad++ferYsaOKFy+uJk2aSJJ27dql7t2765577lGhQoXk7++vnj176vz5807PlXaMgwcPqnPnzvLx8VHp0qU1YsQIGWP0559/qk2bNvL29pa/v78mTZqUqa/dtWvX9NZbb+nee++Vp6enKlWqpH/9619Ol7ZsNptmzZqly5cvO75Ws2fPztTx07i4uOjhhx+WJB07dkzS9dftiSee0KpVq1S/fn15eXnp448/dmy7cQ5RXFycBgwYoEqVKsnT01Ply5dX165dde7cOceYpKQkjRo1SlWqVJGnp6cCAwM1dOjQTF2qGzFihEqUKKHPP//cKQylCQsL0xNPPOG0zm6365133lH58uVVqFAhtWzZUocPH3Ya8+OPP+rZZ59VhQoVHDUNGDBAV65ccRrXvXt3FS1aVCdPnlTbtm1VtGhRlS5dWoMHD1ZqaqrT2PPnz6tLly7y9vaWr6+vunXrpp07d2b42uzfv1/PPPOMSpQooUKFCql+/fpaunSp05jb/cwA2cEZIiAHxMfHO/2iS5OZ/6GPHj1akZGRevHFF9WgQQMlJCRo69at+vXXX/Xoo4+qd+/eOnXqlKKiovTll1867WuM0VNPPaV169YpIiJCdevW1apVqzRkyBCdPHlSU6ZMcYzt3r27FixYoC5duqhRo0b64Ycf1Lp165vW9eyzz6pq1ap69913HeEqKipKv//+u3r06CF/f3/t3btXn3zyifbu3auff/7ZKahJ0vPPP6+aNWtq3LhxWr58ud5++22VKFFCH3/8sR555BGNHz9ec+fO1eDBg/Xggw+qWbNmt/xavfjii5ozZ46eeeYZDRo0SJs3b1ZkZKR+++03/e9//5Mkffnll/rkk0+0ZcsWx2Wwhx566Lavw42OHDkiSSpZsqRj3YEDB9ShQwf17t1bL730kqpXr57hvomJiWratKl+++039ezZUw888IDOnTunpUuX6sSJEypVqpTsdrueeuop/fTTT+rVq5dq1qyp3bt3a8qUKTp48KAWL15809oOHTqk/fv3q2fPnipWrFimexo3bpxcXFw0ePBgxcfHa8KECerUqZM2b97sGLNw4UL99ddf6tOnj0qWLKktW7Zo+vTpOnHihBYuXOh0vNTUVIWFhalhw4Z67733tGbNGk2aNEn33nuv+vTpI+l6CHvyySe1ZcsW9enTRzVq1NCSJUvUrVu3dPXt3btXjRs3Vrly5fTGG2+oSJEiWrBggdq2batvv/1W7dq1k3T7nxkgWwyAbJs1a5aRdMvHfffd57RPxYoVTbdu3RzLQUFBpnXr1rd8nr59+5qMflwXL15sJJm3337baf0zzzxjbDabOXz4sDHGmG3bthlJ5vXXX3ca1717dyPJjBo1yrFu1KhRRpLp0KFDuuf766+/0q37+uuvjSSzfv36dMfo1auXY921a9dM+fLljc1mM+PGjXOsv3jxovHy8nL6mmRkx44dRpJ58cUXndYPHjzYSDJr1651rOvWrZspUqTILY9349izZ8+as2fPmsOHD5t3333X2Gw2c//99zvGVaxY0UgyK1euTHeMG1/TkSNHGklm0aJF6cba7XZjjDFffvmlcXFxMT/++KPT9pkzZxpJZsOGDTetecmSJUaSmTJlSqZ6XLdunZFkatasaZKSkhzrp02bZiSZ3bt3O9Zl9BpHRkYam81m/vjjD8e6bt26GUlm7NixTmPr1atngoODHcvffvutkWSmTp3qWJeammoeeeQRI8nMmjXLsb5ly5amTp065urVq451drvdPPTQQ6Zq1aqOdZn5mQGyiktmQA6YMWOGoqKi0j3uv//+2+7r6+urvXv36tChQ1l+3u+++06urq7q37+/0/pBgwbJGKMVK1ZIklauXCnp+tyYv3v11VdveuyXX3453TovLy/H369evapz586pUaNGkqRff/013fgXX3zR8XdXV1fVr19fxhhFREQ41vv6+qp69er6/fffb1qLdL1XSRo4cKDT+kGDBkmSli9ffsv9b+Xy5csqXbq0SpcurSpVquhf//qXQkJCHGed0lSuXFlhYWG3Pd63336roKAgxxmNv0s7i7Zw4ULVrFlTNWrU0Llz5xyPRx55RJK0bt26mx4/ISFBkrJ0dkiSevTo4TSvqmnTppLk9LX/+2t8+fJlnTt3Tg899JCMMdq+fXu6Y974fdK0aVOn461cuVLu7u566aWXHOtcXFzUt29fp/0uXLigtWvX6rnnntOlS5ccX4/z588rLCxMhw4d0smTJyXd2c8McDNcMgNyQIMGDVS/fv1064sXL57hpbS/Gzt2rNq0aaNq1aqpdu3aCg8PV5cuXTIVpv744w8FBASk+8VYs2ZNx/a0P11cXFS5cmWncVWqVLnpsW8cK13/pTVmzBjNmzdPZ86ccdoWHx+fbnyFChWcln18fFSoUCGVKlUq3fob5yHdKK2HG2v29/eXr6+vo9fsKFSokP7v//5P0vVJ75UrV1b58uXTjcvoa5KRI0eOqH379rccc+jQIf32228qXbp0httv/Pr+nbe3tyTp0qVLmaonzY2vR/HixSVJFy9edKw7fvy4Ro4cqaVLlzqtl9K/xoUKFUpXf/HixZ32++OPP1S2bFkVLlzYadyNr+Phw4dljNGIESM0YsSIDOs/c+aMypUrd0c/M8DNEIiAfNasWTMdOXJES5Ys0erVq/Xpp59qypQpmjlzptMZlrz29zMFaZ577jlt3LhRQ4YMUd26dVW0aFHZ7XaFh4fLbrenG+/q6pqpdZLSTQK/mRvnKeUEV1dXhYaG3nZcRl+T7LLb7apTp44mT56c4fbAwMCb7lujRg1J0u7du7P0nLf72qempurRRx/VhQsXNGzYMNWoUUNFihTRyZMn1b1793Sv8c2Olx1pxx48ePBNz8KlhaiC+jODuxuBCCgASpQooR49eqhHjx5KTExUs2bNNHr0aMc/7jcLARUrVtSaNWt06dIlp7NE+/fvd2xP+9Nut+vo0aOqWrWqY9yNdxjdysWLFxUdHa0xY8Zo5MiRjvV5ddkirYdDhw45zoBJUmxsrOLi4hy9FgT33nuv9uzZc9sxO3fuVMuWLbMc8qpVq6bq1atryZIlmjZtmooWLXon5Trs3r1bBw8e1Jw5c9S1a1fH+ju5e6tixYpat26d/vrrL6ezRDd+791zzz2SJHd390yF09v9zABZxRwiIJ/deKmoaNGiqlKlitOt12nvARQXF+c0tlWrVkpNTdUHH3zgtH7KlCmy2Wx6/PHHJcnxP+4PP/zQadz06dMzXWfa2YAbz+RMnTo108e4E61atcrw+dLOsNzqjrm81r59e+3cuTPdHCTp/3/9nnvuOZ08eVL/+c9/0o25cuWKLl++fMvnGDNmjM6fP68XX3xR165dS7d99erVWrZsWZbqzug1NsZo2rRpWTrO34WFhSklJcWpT7vd7ngriTRlypTRww8/rI8//linT59Od5yzZ886/p6ZnxkgqzhDBOSzWrVq6eGHH1ZwcLBKlCihrVu36ptvvlG/fv0cY4KDgyVJ/fv3V1hYmFxdXfXCCy/oySefVIsWLfTvf/9bx44dU1BQkFavXq0lS5bo9ddf17333uvYv3379po6darOnz/vuO3+4MGDkjJ3Gcrb21vNmjXThAkTlJKSonLlymn16tU6evRoLnxV0gsKClK3bt30ySefKC4uTs2bN9eWLVs0Z84ctW3bVi1atMiTOjJjyJAh+uabb/Tss8+qZ8+eCg4O1oULF7R06VLNnDlTQUFB6tKlixYsWKCXX35Z69atU+PGjZWamqr9+/drwYIFjvc7upnnn39eu3fv1jvvvKPt27erQ4cOjneqXrlypaKjo/XVV19lqe4aNWro3nvv1eDBg3Xy5El5e3vr22+/TTeXKCvatm2rBg0aaNCgQTp8+LBq1KihpUuX6sKFC5Kcv/dmzJihJk2aqE6dOnrppZd0zz33KDY2Vps2bdKJEye0c+dOSZn7mQGyLL9ubwP+CdJuu//ll18y3N68efPb3nb/9ttvmwYNGhhfX1/j5eVlatSoYd555x2TnJzsGHPt2jXz6quvmtKlSxubzeZ0C/6lS5fMgAEDTEBAgHF3dzdVq1Y1EydOdNzeneby5cumb9++pkSJEqZo0aKmbdu25sCBA0aS023wabfMnz17Nl0/J06cMO3atTO+vr7Gx8fHPPvss+bUqVM3vXX/xmPc7Hb4jL5OGUlJSTFjxowxlStXNu7u7iYwMNAMHz7c6TbtWz1PRjI7tmLFije91fvG19QYY86fP2/69etnypUrZzw8PEz58uVNt27dzLlz5xxjkpOTzfjx4819991nPD09TfHixU1wcLAZM2aMiY+Pz1T90dHRpk2bNqZMmTLGzc3NlC5d2jz55JNmyZIljjFpt90vXLjQad+jR4+mu/V93759JjQ01BQtWtSUKlXKvPTSS2bnzp3pxt3s65b22v/d2bNnTceOHU2xYsWMj4+P6d69u9mwYYORZObNm+c09siRI6Zr167G39/fuLu7m3LlypknnnjCfPPNN44xmfmZAbLKZkwmZzIC+MfZsWOH6tWrp//+97/q1KlTfpcDC1m8eLHatWunn376SY0bN87vcgDmEAFWceNHL0jX5+O4uLjc9h2igTtx4/deamqqpk+fLm9vbz3wwAP5VBXgjDlEgEVMmDBB27ZtU4sWLeTm5qYVK1ZoxYoV6tWr1y1v8Qbu1KuvvqorV64oJCRESUlJWrRokTZu3Kh33303R9/KALgTXDIDLCIqKkpjxozRvn37lJiYqAoVKqhLly7697//LTc3/m+E3PPVV19p0qRJOnz4sK5evaoqVaqoT58+TIJGgUIgAgAAlsccIgAAYHkEIgAAYHlMHMgEu92uU6dOqVixYrnyOUoAACDnGWN06dIlBQQEyMXl1ueACESZcOrUKe7CAQDgLvXnn3+qfPnytxxDIMqEtA/N/PPPP+Xt7Z3P1QAAgMxISEhQYGCg04df3wyBKBPSLpN5e3sTiAAAuMtkZroLk6oBAIDlEYgAAIDlEYgAAIDlEYgAAIDlEYgAAIDlEYgAAIDlEYgAAIDlEYgAAIDlEYgAAIDlEYgAAIDlEYgAAIDlEYgAAIDlEYgAAIDlEYgAAIDlEYgAAIDlueV3AQD+WSq9sTy/S8iyY+Na53cJAPIZZ4gAAIDlEYgAAIDlEYgAAIDlEYgAAIDlEYgAAIDlEYgAAIDlEYgAAIDlEYgAAIDlEYgAAIDlEYgAAIDlEYgAAIDlEYgAAIDlEYgAAIDlEYgAAIDlEYgAAIDlEYgAAIDlEYgAAIDlEYgAAIDlEYgAAIDlEYgAAIDlEYgAAIDlEYgAAIDlEYgAAIDlEYgAAIDlEYgAAIDlEYgAAIDlEYgAAIDlEYgAAIDlEYgAAIDlEYgAAIDlEYgAAIDlEYgAAIDlEYgAAIDl5WsgioyM1IMPPqhixYqpTJkyatu2rQ4cOOA05urVq+rbt69KliypokWLqn379oqNjXUac/z4cbVu3VqFCxdWmTJlNGTIEF27ds1pzPfff68HHnhAnp6eqlKlimbPnp3b7QEAgLtEvgaiH374QX379tXPP/+sqKgopaSk6LHHHtPly5cdYwYMGKD/+7//08KFC/XDDz/o1KlTevrppx3bU1NT1bp1ayUnJ2vjxo2aM2eOZs+erZEjRzrGHD16VK1bt1aLFi20Y8cOvf7663rxxRe1atWqPO0XAAAUTDZjjMnvItKcPXtWZcqU0Q8//KBmzZopPj5epUuX1ldffaVnnnlGkrR//37VrFlTmzZtUqNGjbRixQo98cQTOnXqlPz8/CRJM2fO1LBhw3T27Fl5eHho2LBhWr58ufbs2eN4rhdeeEFxcXFauXLlbetKSEiQj4+P4uPj5e3tnTvNA/8Qld5Ynt8lZNmxca3zuwQAuSArv78L1Byi+Ph4SVKJEiUkSdu2bVNKSopCQ0MdY2rUqKEKFSpo06ZNkqRNmzapTp06jjAkSWFhYUpISNDevXsdY/5+jLQxaccAAADW5pbfBaSx2+16/fXX1bhxY9WuXVuSFBMTIw8PD/n6+jqN9fPzU0xMjGPM38NQ2va0bbcak5CQoCtXrsjLy8tpW1JSkpKSkhzLCQkJd94gAAAosArMGaK+fftqz549mjdvXn6XosjISPn4+DgegYGB+V0SAADIRQUiEPXr10/Lli3TunXrVL58ecd6f39/JScnKy4uzml8bGys/P39HWNuvOssbfl2Y7y9vdOdHZKk4cOHKz4+3vH4888/77hHAABQcOVrIDLGqF+/fvrf//6ntWvXqnLlyk7bg4OD5e7urujoaMe6AwcO6Pjx4woJCZEkhYSEaPfu3Tpz5oxjTFRUlLy9vVWrVi3HmL8fI21M2jFu5OnpKW9vb6cHAAD458rXOUR9+/bVV199pSVLlqhYsWKOOT8+Pj7y8vKSj4+PIiIiNHDgQJUoUULe3t569dVXFRISokaNGkmSHnvsMdWqVUtdunTRhAkTFBMTozfffFN9+/aVp6enJOnll1/WBx98oKFDh6pnz55au3atFixYoOXL7767YQAAQM7L1zNEH330keLj4/Xwww+rbNmyjsf8+fMdY6ZMmaInnnhC7du3V7NmzeTv769FixY5tru6umrZsmVydXVVSEiIOnfurK5du2rs2LGOMZUrV9by5csVFRWloKAgTZo0SZ9++qnCwsLytF8AAFAwFaj3ISqoeB8iIPN4HyIABcVd+z5EAAAA+YFABAAALI9ABAAALI9ABAAALI9ABAAALI9ABAAALI9ABAAALI9ABAAALI9ABAAALI9ABAAALI9ABAAALI9ABAAALI9ABAAALI9ABAAALI9ABAAALI9ABAAALI9ABAAALI9ABAAALI9ABAAALI9ABAAALI9ABAAALI9ABAAALI9ABAAALI9ABAAALI9ABAAALI9ABAAALI9ABAAALI9ABAAALI9ABAAALI9ABAAALI9ABAAALI9ABAAALI9ABAAALI9ABAAALI9ABAAALI9ABAAALI9ABAAALI9ABAAALI9ABAAALI9ABAAALI9ABAAALI9ABAAALI9ABAAALI9ABAAALI9ABAAALI9ABAAALI9ABAAALI9ABAAALI9ABAAALI9ABAAALI9ABAAALI9ABAAALI9ABAAALI9ABAAALI9ABAAALI9ABAAALI9ABAAALI9ABAAALI9ABAAALI9ABAAALI9ABAAALI9ABAAALI9ABAAALI9ABAAALI9ABAAALI9ABAAALI9ABAAALI9ABAAALC9fA9H69ev15JNPKiAgQDabTYsXL3ba3r17d9lsNqdHeHi405gLFy6oU6dO8vb2lq+vryIiIpSYmOg0ZteuXWratKkKFSqkwMBATZgwIbdbAwAAd5F8DUSXL19WUFCQZsyYcdMx4eHhOn36tOPx9ddfO23v1KmT9u7dq6ioKC1btkzr169Xr169HNsTEhL02GOPqWLFitq2bZsmTpyo0aNH65NPPsm1vgAAwN3FLT+f/PHHH9fjjz9+yzGenp7y9/fPcNtvv/2mlStX6pdfflH9+vUlSdOnT1erVq303nvvKSAgQHPnzlVycrI+//xzeXh46L777tOOHTs0efJkp+AEAACsq8DPIfr+++9VpkwZVa9eXX369NH58+cd2zZt2iRfX19HGJKk0NBQubi4aPPmzY4xzZo1k4eHh2NMWFiYDhw4oIsXL2b4nElJSUpISHB6AACAf64CHYjCw8P1xRdfKDo6WuPHj9cPP/ygxx9/XKmpqZKkmJgYlSlTxmkfNzc3lShRQjExMY4xfn5+TmPSltPG3CgyMlI+Pj6OR2BgYE63BgAACpB8vWR2Oy+88ILj73Xq1NH999+ve++9V99//71atmyZa887fPhwDRw40LGckJBAKAIA4B+sQJ8hutE999yjUqVK6fDhw5Ikf39/nTlzxmnMtWvXdOHCBce8I39/f8XGxjqNSVu+2dwkT09PeXt7Oz0AAMA/110ViE6cOKHz58+rbNmykqSQkBDFxcVp27ZtjjFr166V3W5Xw4YNHWPWr1+vlJQUx5ioqChVr15dxYsXz9sGAABAgZSvgSgxMVE7duzQjh07JElHjx7Vjh07dPz4cSUmJmrIkCH6+eefdezYMUVHR6tNmzaqUqWKwsLCJEk1a9ZUeHi4XnrpJW3ZskUbNmxQv3799MILLyggIECS1LFjR3l4eCgiIkJ79+7V/PnzNW3aNKdLYgAAwNryNRBt3bpV9erVU7169SRJAwcOVL169TRy5Ei5urpq165deuqpp1StWjVFREQoODhYP/74ozw9PR3HmDt3rmrUqKGWLVuqVatWatKkidN7DPn4+Gj16tU6evSogoODNWjQII0cOZJb7gEAgIPNGGPyu4iCLiEhQT4+PoqPj2c+EXAbld5Ynt8lZNmxca3zuwQAuSArv7/vqjlEAAAAuYFABAAALI9ABAAALI9ABAAALI9ABAAALI9ABAAALI9ABAAALI9ABAAALI9ABAAALI9ABAAALC9bgej333/P6ToAAADyTbYCUZUqVdSiRQv997//1dWrV3O6JgAAgDyVrUD066+/6v7779fAgQPl7++v3r17a8uWLTldGwAAQJ7IViCqW7eupk2bplOnTunzzz/X6dOn1aRJE9WuXVuTJ0/W2bNnc7pOAACAXHNHk6rd3Nz09NNPa+HChRo/frwOHz6swYMHKzAwUF27dtXp06dzqk4AAIBcc0eBaOvWrXrllVdUtmxZTZ48WYMHD9aRI0cUFRWlU6dOqU2bNjlVJwAAQK5xy85OkydP1qxZs3TgwAG1atVKX3zxhVq1aiUXl+v5qnLlypo9e7YqVaqUk7UCAADkimwFoo8++kg9e/ZU9+7dVbZs2QzHlClTRp999tkdFQcAAJAXshWIDh06dNsxHh4e6tatW3YODwAAkKeyNYdo1qxZWrhwYbr1Cxcu1Jw5c+64KAAAgLyUrUAUGRmpUqVKpVtfpkwZvfvuu3dcFAAAQF7KViA6fvy4KleunG59xYoVdfz48TsuCgAAIC9lKxCVKVNGu3btSrd+586dKlmy5B0XBQAAkJeyFYg6dOig/v37a926dUpNTVVqaqrWrl2r1157TS+88EJO1wgAAJCrsnWX2VtvvaVjx46pZcuWcnO7fgi73a6uXbsyhwgAANx1shWIPDw8NH/+fL311lvauXOnvLy8VKdOHVWsWDGn6wMAAMh12QpEaapVq6Zq1arlVC0AAAD5IluBKDU1VbNnz1Z0dLTOnDkju93utH3t2rU5UhwAAEBeyFYgeu211zR79my1bt1atWvXls1my+m6AAAA8ky2AtG8efO0YMECtWrVKqfrAQAAyHPZuu3ew8NDVapUyelaAAAA8kW2AtGgQYM0bdo0GWNyuh4AAIA8l61LZj/99JPWrVunFStW6L777pO7u7vT9kWLFuVIcQAAAHkhW4HI19dX7dq1y+laAAAA8kW2AtGsWbNyug4AAIB8k605RJJ07do1rVmzRh9//LEuXbokSTp16pQSExNzrDgAAIC8kK0zRH/88YfCw8N1/PhxJSUl6dFHH1WxYsU0fvx4JSUlaebMmTldJwAAQK7J1hmi1157TfXr19fFixfl5eXlWN+uXTtFR0fnWHEAAAB5IVtniH788Udt3LhRHh4eTusrVaqkkydP5khhAAAAeSVbZ4jsdrtSU1PTrT9x4oSKFSt2x0UBAADkpWwFoscee0xTp051LNtsNiUmJmrUqFF8nAcAALjrZOuS2aRJkxQWFqZatWrp6tWr6tixow4dOqRSpUrp66+/zukaAQAAclW2AlH58uW1c+dOzZs3T7t27VJiYqIiIiLUqVMnp0nWAAAAd4NsBSJJcnNzU+fOnXOyFgAAgHyRrUD0xRdf3HJ7165ds1UMAABAfshWIHrttdecllNSUvTXX3/Jw8NDhQsXJhABAIC7SrbuMrt48aLTIzExUQcOHFCTJk2YVA0AAO462f4ssxtVrVpV48aNS3f2CAAAoKDLsUAkXZ9oferUqZw8JAAAQK7L1hyipUuXOi0bY3T69Gl98MEHaty4cY4UBgAAkFeyFYjatm3rtGyz2VS6dGk98sgjmjRpUk7UBQAAkGeyFYjsdntO1wEAAJBvcnQOEQAAwN0oW2eIBg4cmOmxkydPzs5TAAAA5JlsBaLt27dr+/btSklJUfXq1SVJBw8elKurqx544AHHOJvNljNVAgAA5KJsBaInn3xSxYoV05w5c1S8eHFJ19+ssUePHmratKkGDRqUo0UCAADkpmzNIZo0aZIiIyMdYUiSihcvrrfffpu7zAAAwF0nW4EoISFBZ8+eTbf+7NmzunTp0h0XBQAAkJeyFYjatWunHj16aNGiRTpx4oROnDihb7/9VhEREXr66adzukYAAIBcla05RDNnztTgwYPVsWNHpaSkXD+Qm5siIiI0ceLEHC0QAAAgt2UrEBUuXFgffvihJk6cqCNHjkiS7r33XhUpUiRHiwMAAMgLd/TGjKdPn9bp06dVtWpVFSlSRMaYnKoLAAAgz2QrEJ0/f14tW7ZUtWrV1KpVK50+fVqSFBERwS33AADgrpOtQDRgwAC5u7vr+PHjKly4sGP9888/r5UrV+ZYcQAAAHkhW3OIVq9erVWrVql8+fJO66tWrao//vgjRwoDAADIK9k6Q3T58mWnM0NpLly4IE9PzzsuCgAAIC9lKxA1bdpUX3zxhWPZZrPJbrdrwoQJatGiRY4VBwAAkBeyFYgmTJigTz75RI8//riSk5M1dOhQ1a5dW+vXr9f48eMzfZz169frySefVEBAgGw2mxYvXuy03RijkSNHqmzZsvLy8lJoaKgOHTrkNObChQvq1KmTvL295evrq4iICCUmJjqN2bVrl5o2bapChQopMDBQEyZMyE7bAADgHypbc4hq166tgwcP6oMPPlCxYsWUmJiop59+Wn379lXZsmUzfZzLly8rKChIPXv2zPAdridMmKD3339fc+bMUeXKlTVixAiFhYVp3759KlSokCSpU6dOOn36tKKiopSSkqIePXqoV69e+uqrryRd/5iRxx57TKGhoZo5c6Z2796tnj17ytfXV7169cpO+0CeqfTG8vwuAQAswWay+OZBKSkpCg8P18yZM1W1atWcK8Rm0//+9z+1bdtW0vWzQwEBARo0aJAGDx4sSYqPj5efn59mz56tF154Qb/99ptq1aqlX375RfXr15ckrVy5Uq1atdKJEycUEBCgjz76SP/+978VExMjDw8PSdIbb7yhxYsXa//+/ZmqLSEhQT4+PoqPj5e3t3eO9QzcDoEobxwb1zq/SwCQC7Ly+zvLl8zc3d21a9eubBeXWUePHlVMTIxCQ0Md63x8fNSwYUNt2rRJkrRp0yb5+vo6wpAkhYaGysXFRZs3b3aMadasmSMMSVJYWJgOHDigixcvZvjcSUlJSkhIcHoAAIB/rmzNIercubM+++yznK7FSUxMjCTJz8/Pab2fn59jW0xMjMqUKeO03c3NTSVKlHAak9Ex/v4cN4qMjJSPj4/jERgYeOcNAQCAAitbc4iuXbumzz//XGvWrFFwcHC6zzCbPHlyjhSXX4YPH66BAwc6lhMSEghFAAD8g2UpEP3++++qVKmS9uzZowceeECSdPDgQacxNpstRwrz9/eXJMXGxjpN1I6NjVXdunUdY86cOeO037Vr13ThwgXH/v7+/oqNjXUak7acNuZGnp6evJ8SAAAWkqVLZlWrVtW5c+e0bt06rVu3TmXKlNG8efMcy+vWrdPatWtzpLDKlSvL399f0dHRjnUJCQnavHmzQkJCJEkhISGKi4vTtm3bHGPWrl0ru92uhg0bOsasX79eKSkpjjFRUVGqXr26ihcvniO1AgCAu1uWAtGNN6StWLFCly9fzvaTJyYmaseOHdqxY4ek6xOpd+zYoePHj8tms+n111/X22+/raVLl2r37t3q2rWrAgICHHei1axZU+Hh4XrppZe0ZcsWbdiwQf369dMLL7yggIAASVLHjh3l4eGhiIgI7d27V/Pnz9e0adOcLokBAABry9YcojRZvGM/na1btzq9s3VaSOnWrZtmz56toUOH6vLly+rVq5fi4uLUpEkTrVy50vEeRJI0d+5c9evXTy1btpSLi4vat2+v999/37Hdx8dHq1evVt++fRUcHKxSpUpp5MiRvAcRAABwyNL7ELm6uiomJkalS5eWJBUrVky7du1S5cqVc63AgoD3IUJ+4X2I8gbvQwT8M2Xl93eWzhAZY9S9e3fHhOOrV6/q5ZdfTneX2aJFi7JYMgAAQP7JUiDq1q2b03Lnzp1ztBgAAID8kKVANGvWrNyqAwAAIN/c0aRqAPgnuBvnajHvCchZ2froDgAAgH8SAhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALC8Ah2IRo8eLZvN5vSoUaOGY/vVq1fVt29flSxZUkWLFlX79u0VGxvrdIzjx4+rdevWKly4sMqUKaMhQ4bo2rVred0KAAAowNzyu4Dbue+++7RmzRrHspvb/y95wIABWr58uRYuXCgfHx/169dPTz/9tDZs2CBJSk1NVevWreXv76+NGzfq9OnT6tq1q9zd3fXuu+/meS8AAKBgKvCByM3NTf7+/unWx8fH67PPPtNXX32lRx55RJI0a9Ys1axZUz///LMaNWqk1atXa9++fVqzZo38/PxUt25dvfXWWxo2bJhGjx4tDw+PvG4HAAAUQAX6kpkkHTp0SAEBAbrnnnvUqVMnHT9+XJK0bds2paSkKDQ01DG2Ro0aqlChgjZt2iRJ2rRpk+rUqSM/Pz/HmLCwMCUkJGjv3r03fc6kpCQlJCQ4PQAAwD9XgQ5EDRs21OzZs7Vy5Up99NFHOnr0qJo2bapLly4pJiZGHh4e8vX1ddrHz89PMTExkqSYmBinMJS2PW3bzURGRsrHx8fxCAwMzNnGAABAgVKgL5k9/vjjjr/ff//9atiwoSpWrKgFCxbIy8sr1553+PDhGjhwoGM5ISGBUAQAwD9YgT5DdCNfX19Vq1ZNhw8flr+/v5KTkxUXF+c0JjY21jHnyN/fP91dZ2nLGc1LSuPp6Slvb2+nBwAA+Oe6qwJRYmKijhw5orJlyyo4OFju7u6Kjo52bD9w4ICOHz+ukJAQSVJISIh2796tM2fOOMZERUXJ29tbtWrVyvP6AQBAwVSgL5kNHjxYTz75pCpWrKhTp05p1KhRcnV1VYcOHeTj46OIiAgNHDhQJUqUkLe3t1599VWFhISoUaNGkqTHHntMtWrVUpcuXTRhwgTFxMTozTffVN++feXp6ZnP3QEAgIKiQAeiEydOqEOHDjp//rxKly6tJk2a6Oeff1bp0qUlSVOmTJGLi4vat2+vpKQkhYWF6cMPP3Ts7+rqqmXLlqlPnz4KCQlRkSJF1K1bN40dOza/WgIAAAWQzRhj8ruIgi4hIUE+Pj6Kj49nPhHyVKU3lud3CSigjo1rnd8lAAVeVn5/31VziAAAAHIDgQgAAFgegQgAAFgegQgAAFgegQgAAFgegQgAAFgegQgAAFgegQgAAFgegQgAAFgegQgAAFgegQgAAFgegQgAAFgegQgAAFgegQgAAFgegQgAAFgegQgAAFgegQgAAFgegQgAAFgegQgAAFgegQgAAFgegQgAAFgegQgAAFgegQgAAFgegQgAAFgegQgAAFgegQgAAFgegQgAAFgegQgAAFgegQgAAFgegQgAAFgegQgAAFgegQgAAFgegQgAAFgegQgAAFgegQgAAFgegQgAAFgegQgAAFgegQgAAFieW34XAOSVSm8sz+8SAAAFFGeIAACA5RGIAACA5RGIAACA5RGIAACA5RGIAACA5XGXGQDche7WuyaPjWud3yUAGeIMEQAAsDwCEQAAsDwCEQAAsDwCEQAAsDwCEQAAsDwCEQAAsDwCEQAAsDwCEQAAsDwCEQAAsDwCEQAAsDwCEQAAsDwCEQAAsDwCEQAAsDwCEQAAsDwCEQAAsDwCEQAAsDwCEQAAsDwCEQAAsDwCEQAAsDy3/C4Ad6dKbyzP7xIAAMgxnCECAACWRyACAACWZ6lLZjNmzNDEiRMVExOjoKAgTZ8+XQ0aNMjvsgDAMu7Gy+3HxrXO7xKQByxzhmj+/PkaOHCgRo0apV9//VVBQUEKCwvTmTNn8rs0AACQz2zGGJPfReSFhg0b6sEHH9QHH3wgSbLb7QoMDNSrr76qN95445b7JiQkyMfHR/Hx8fL29s7x2u7G/zEBgFVwhujulZXf35a4ZJacnKxt27Zp+PDhjnUuLi4KDQ3Vpk2b8rEyAEBBdzf+p5UQl3WWCETnzp1Tamqq/Pz8nNb7+flp//796cYnJSUpKSnJsRwfHy/petLMDfakv3LluAAAa6owYGF+l5Ble8aE5fgx035vZ+ZimCUCUVZFRkZqzJgx6dYHBgbmQzUAAPzz+UzNvWNfunRJPj4+txxjiUBUqlQpubq6KjY21ml9bGys/P39040fPny4Bg4c6Fi22+26cOGCSpYsKZvNluv1ZkdCQoICAwP1559/5so8p4KInun5n8hq/Ur0TM+5xxijS5cuKSAg4LZjLRGIPDw8FBwcrOjoaLVt21bS9ZATHR2tfv36pRvv6ekpT09Pp3W+vr55UOmd8/b2tswPVxp6tgar9Wy1fiV6toq87vl2Z4bSWCIQSdLAgQPVrVs31a9fXw0aNNDUqVN1+fJl9ejRI79LAwAA+cwygej555/X2bNnNXLkSMXExKhu3bpauXJluonWAADAeiwTiCSpX79+GV4i+yfw9PTUqFGj0l3q+yejZ2uwWs9W61eiZ6so6D1b5o0ZAQAAbsYyH90BAABwMwQiAABgeQQiAABgeQQiAABgeQSiu8iFCxfUqVMneXt7y9fXVxEREUpMTLzlPlevXlXfvn1VsmRJFS1aVO3bt3d6x+7z588rPDxcAQEB8vT0VGBgoPr165drn9uWVbnR886dO9WhQwcFBgbKy8tLNWvW1LRp03K7lUzJjX4lqX///goODpanp6fq1q2bix3c3owZM1SpUiUVKlRIDRs21JYtW245fuHChapRo4YKFSqkOnXq6LvvvnPabozRyJEjVbZsWXl5eSk0NFSHDh3KzRayLKd7XrRokR577DHHu+fv2LEjF6vPnpzsOSUlRcOGDVOdOnVUpEgRBQQEqGvXrjp16lRut5ElOf06jx49WjVq1FCRIkVUvHhxhYaGavPmzbnZQpbkdL9/9/LLL8tms2nq1Kk5XPUtGNw1wsPDTVBQkPn555/Njz/+aKpUqWI6dOhwy31efvllExgYaKKjo83WrVtNo0aNzEMPPeTYfuHCBfPhhx+aX375xRw7dsysWbPGVK9e/bbHzSu50fNnn31m+vfvb77//ntz5MgR8+WXXxovLy8zffr03G7ntnKjX2OMefXVV80HH3xgunTpYoKCgnKxg1ubN2+e8fDwMJ9//rnZu3eveemll4yvr6+JjY3NcPyGDRuMq6urmTBhgtm3b5958803jbu7u9m9e7djzLhx44yPj49ZvHix2blzp3nqqadM5cqVzZUrV/KqrVvKjZ6/+OILM2bMGPOf//zHSDLbt2/Po24yJ6d7jouLM6GhoWb+/Plm//79ZtOmTaZBgwYmODg4L9u6pdx4nefOnWuioqLMkSNHzJ49e0xERITx9vY2Z86cyau2bio3+k2zaNEiExQUZAICAsyUKVNyuZP/j0B0l9i3b5+RZH755RfHuhUrVhibzWZOnjyZ4T5xcXHG3d3dLFy40LHut99+M5LMpk2bbvpc06ZNM+XLl8+54rMpL3t+5ZVXTIsWLXKu+GzIi35HjRqVr4GoQYMGpm/fvo7l1NRUExAQYCIjIzMc/9xzz5nWrVs7rWvYsKHp3bu3McYYu91u/P39zcSJEx3b4+LijKenp/n6669zoYOsy+me/+7o0aMFMhDlZs9ptmzZYiSZP/74I2eKvkN50XN8fLyRZNasWZMzRd+B3Or3xIkTply5cmbPnj2mYsWKeRqIuGR2l9i0aZN8fX1Vv359x7rQ0FC5uLjc9BTqtm3blJKSotDQUMe6GjVqqEKFCtq0aVOG+5w6dUqLFi1S8+bNc7aBbMirniUpPj5eJUqUyLnisyEv+80PycnJ2rZtm1OtLi4uCg0NvWmtmzZtchovSWFhYY7xR48eVUxMjNMYHx8fNWzYsED0nxs9F3R51XN8fLxsNluB+JzJvOg5OTlZn3zyiXx8fBQUFJRzxWdDbvVrt9vVpUsXDRkyRPfdd1/uFH8LBKK7RExMjMqUKeO0zs3NTSVKlFBMTMxN9/Hw8Ej3D4afn1+6fTp06KDChQurXLly8vb21qeffpqj9WdHbvecZuPGjZo/f7569eqVI3VnV171m1/OnTun1NTUdB+Xc6taY2Jibjk+7c+sHDMv5UbPBV1e9Hz16lUNGzZMHTp0KBAfjJqbPS9btkxFixZVoUKFNGXKFEVFRalUqVI520AW5Va/48ePl5ubm/r375/zRWcCgSifvfHGG7LZbLd87N+/P9frmDJlin799VctWbJER44c0cCBA3PtuQpKz5K0Z88etWnTRqNGjdJjjz2WK89RkPoF7nYpKSl67rnnZIzRRx99lN/l5LoWLVpox44d2rhxo8LDw/Xcc8/pzJkz+V1Wjtu2bZumTZum2bNny2az5UsNlvoss4Jo0KBB6t69+y3H3HPPPfL390/3Q3Dt2jVduHBB/v7+Ge7n7++v5ORkxcXFOZ1BiI2NTbePv7+//P39VaNGDZUoUUJNmzbViBEjVLZs2Wz1dSsFped9+/apZcuW6tWrl958881s9ZIZBaXf/FaqVCm5urqmuwPuVrX6+/vfcnzan7GxsU7fq7Gxsfl+N52UOz0XdLnZc1oY+uOPP7R27doCcXZIyt2eixQpoipVqqhKlSpq1KiRqlatqs8++0zDhw/P2SayIDf6/fHHH3XmzBlVqFDBsT01NVWDBg3S1KlTdezYsZxtIgOcIcpnpUuXVo0aNW758PDwUEhIiOLi4rRt2zbHvmvXrpXdblfDhg0zPHZwcLDc3d0VHR3tWHfgwAEdP35cISEhN63JbrdLkpKSknKoS2cFoee9e/eqRYsW6tatm955551c6TNNQei3IPDw8FBwcLBTrXa7XdHR0TetNSQkxGm8JEVFRTnGV65cWf7+/k5jEhIStHnz5gLRf270XNDlVs9pYejQoUNas2aNSpYsmTsNZENevs52uz3X/m3OrNzot0uXLtq1a5d27NjheAQEBGjIkCFatWpV7jXzd3k2fRt3LDw83NSrV89s3rzZ/PTTT6Zq1apOt2SfOHHCVK9e3WzevNmx7uWXXzYVKlQwa9euNVu3bjUhISEmJCTEsX358uXm888/N7t37zZHjx41y5YtMzVr1jSNGzfO095uJjd63r17tyldurTp3LmzOX36tONREG5lzY1+jTHm0KFDZvv27aZ3796mWrVqZvv27Wb79u0mKSkpz3oz5vqtup6enmb27Nlm3759plevXsbX19fExMQYY4zp0qWLeeONNxzjN2zYYNzc3Mx7771nfvvtNzNq1KgMb7v39fU1S5YsMbt27TJt2rQpcLfd53TP58+fN9u3bzfLly83ksy8efPM9u3bzenTp/O8v4zkdM/JycnmqaeeMuXLlzc7duxw+rnN6+/hm8npnhMTE83w4cPNpk2bzLFjx8zWrVtNjx49jKenp9mzZ0++9Ph3ufF9faO8vsuMQHQXOX/+vOnQoYMpWrSo8fb2Nj169DCXLl1ybE+7BXfdunWOdVeuXDGvvPKKKV68uClcuLBp166d0z+aa9euNSEhIcbHx8cUKlTIVK1a1QwbNsxcvHgxDzu7udzoedSoUUZSukfFihXzsLOM5Ua/xhjTvHnzDHs+evRoHnX2/02fPt1UqFDBeHh4mAYNGpiff/7Zqc5u3bo5jV+wYIGpVq2a8fDwMPfdd59Zvny503a73W5GjBhh/Pz8jKenp2nZsqU5cOBAXrSSaTnd86xZszJ8PUeNGpUH3WROTvac9n2f0ePvPwv5LSd7vnLlimnXrp0JCAgwHh4epmzZsuapp54yW7Zsyat2biunv69vlNeByGaMMXlzLgoAAKBgYg4RAACwPAIRAACwPAIRAACwPAIRAACwPAIRAACwPAIRAACwPAIRAACwPAIRgAKjUqVKmjp1an6XoYcfflivv/56fpcBIA8RiADkuO7du8tms8lms8nDw0NVqlTR2LFjde3atVvu98svv6hXr165WltycrImTJigoKAgFS5cWKVKlVLjxo01a9YspaSk5OpzAyi4+LR7ALkiPDxcs2bNUlJSkr777jv17dtX7u7uGX5Kd3Jysjw8PFS6dOlcrSk5OVlhYWHauXOn3nrrLTVu3Fje3t76+eef9d5776levXqqW7durtYAoGDiDBGAXOHp6Sl/f39VrFhRffr0UWhoqJYuXSrp+hmktm3b6p133lFAQICqV68uKf0ls7i4OPXu3Vt+fn4qVKiQateurWXLljm2//TTT2ratKm8vLwUGBio/v376/LlyzetaerUqVq/fr2io6PVt29f1a1bV/fcc486duyozZs3q2rVqo6xdrtdQ4cOVYkSJeTv76/Ro0c7HWvy5MmqU6eOihQposDAQL3yyitKTEx0bJ89e7Z8fX21atUq1axZU0WLFlV4eLhOnz7tGHPt2jX1799fvr6+KlmypIYNG6Zu3bqpbdu2TnVERkaqcuXK8vLyUlBQkL755pssvRYAbo9ABCBPeHl5KTk52bEcHR2tAwcOKCoqyinkpLHb7Xr88ce1YcMG/fe//9W+ffs0btw4ubq6SpKOHDmi8PBwtW/fXrt27dL8+fP1008/qV+/fjetYe7cuQoNDVW9evXSbXN3d1eRIkUcy3PmzFGRIkW0efNmTZgwQWPHjlVUVJRju4uLi95//33t3btXc+bM0dq1azV06FCnY/71119677339OWXX2r9+vU6fvy4Bg8e7Ng+fvx4zZ07V7NmzdKGDRuUkJCgxYsXOx0jMjJSX3zxhWbOnKm9e/dqwIAB6ty5s3744Yeb9gkgG/LsY2QBWEa3bt1MmzZtjDHXP40+KirKeHp6msGDBzu2+/n5maSkJKf9/v7p1qtWrTIuLi43/eT6iIgI06tXL6d1P/74o3FxcTFXrlzJcB8vLy/Tv3//29bfvHlz06RJE6d1Dz74oBk2bNhN91m4cKEpWbKkYzntE+kPHz7sWDdjxgzj5+fnWPbz8zMTJ050LF+7ds1UqFDB8bW7evWqKVy4sNm4caPTc0VERJgOHTrctg8AmcccIgC5YtmyZSpatKhSUlJkt9vVsWNHp8tOderUkYeHx03337Fjh8qXL69q1apluH3nzp3atWuX5s6d61hnjJHdbtfRo0dVs2bNdPsYYzJd//333++0XLZsWZ05c8axvGbNGkVGRmr//v1KSEjQtWvXdPXqVf31118qXLiwJKlw4cK69957MzxGfHy8YmNj1aBBA8d2V1dXBQcHy263S5IOHz6sv/76S48++qhTLcnJyRme5QKQfQQiALmiRYsW+uijj+Th4aGAgAC5uTn/c/P3y1MZ8fLyuuX2xMRE9e7dW/3790+3rUKFChnuU61aNe3fv/82lV/n7u7utGyz2RxB5dixY3riiSfUp08fvfPOOypRooR++uknRUREKDk52RGIMjpGVkJZ2pyk5cuXq1y5ck7bPD09M30cALdHIAKQK4oUKaIqVapke//7779fJ06c0MGDBzM8S/TAAw9o3759WXqOjh076l//+pe2b9+e7gxLSkqKkpOTbxvUJGnbtm2y2+2aNGmSXFyuT8VcsGBBpuuQJB8fH/n5+emXX35Rs2bNJEmpqan69ddfHXe61apVS56enjp+/LiaN2+epeMDyBomVQMokJo3b65mzZqpffv2ioqK0tGjR7VixQqtXLlSkjRs2DBt3LhR/fr1044dO3To0CEtWbLklpOqX3/9dTVu3FgtW7bUjBkztHPnTv3+++9asGCBGjVqpEOHDmWqtipVqiglJUXTp0/X77//ri+//FIzZ87Mco+vvvqqIiMjtWTJEh04cECvvfaaLl68KJvNJkkqVqyYBg8erAEDBmjOnDk6cuSIfv31V02fPl1z5szJ8vMBuDkCEYAC69tvv9WDDz6oDh06qFatWho6dKhSU1MlXT+D9MMPP+jgwYNq2rSp6tWrp5EjRyogIOCmx/P09FRUVJSGDh2qjz/+WI0aNdKDDz6o999/X/3791ft2rUzVVdQUJAmT56s8ePHq3bt2po7d64iIyOz3N+wYcPUoUMHde3aVSEhISpatKjCwsJUqFAhx5i33npLI0aMUGRkpGrWrKnw8HAtX75clStXzvLzAbg5m8nKBW0AQK6x2+2qWbOmnnvuOb311lv5XQ5gKcwhAoB88scff2j16tVq3ry5kpKS9MEHH+jo0aPq2LFjfpcGWA6XzAAgn7i4uGj27Nl68MEH1bhxY+3evVtr1qzJ8C0DAOQuLpkBAADL4wwRAACwPAIRAACwPAIRAACwPAIRAACwPAIRAACwPAIRAACwPAIRAACwPAIRAACwPAIRAACwvP8HDO4BFEDbBL4AAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Calculate price changes\n",
    "price_changes = np.diff(data_eco['c'])\n",
    "\n",
    "# Perform histogram analysis\n",
    "histogram, bins = np.histogram(price_changes, bins=10)\n",
    "\n",
    "# Plot histogram\n",
    "plt.bar(bins[:-1], histogram, width=np.diff(bins), align='edge')\n",
    "plt.xlabel('Price Change')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Price Changes')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1873,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.0017 , -0.0019 ,  0.00214, ..., -0.00083, -0.00164, -0.00272])"
      ]
     },
     "execution_count": 1873,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "price_changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1874,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upper bound of the threshold value: 0.04116999999999993\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Determine number of bins\n",
    "number_of_bins = 10\n",
    "\n",
    "# Calculate histogram\n",
    "histogram, bins = np.histogram(price_changes, bins=number_of_bins, range=(0, price_changes.max()))\n",
    "\n",
    "# Sort histogram counts in descending order\n",
    "sorted_histogram = np.sort(histogram)[::-1]\n",
    "\n",
    "# Calculate cumulative sum of sorted histogram counts\n",
    "cumulative_sum = np.cumsum(sorted_histogram)\n",
    "\n",
    "# Find the index where cumulative sum exceeds 85% of the whole count\n",
    "index_85_percent = np.argmax(cumulative_sum >= len(price_changes) * 0.85)\n",
    "\n",
    "# Calculate the upper bound of the threshold value\n",
    "if index_85_percent == 0:\n",
    "    threshold_upper_bound = bins[-1]\n",
    "else:\n",
    "    threshold_upper_bound = bins[index_85_percent]\n",
    "\n",
    "print(\"Upper bound of the threshold value:\", threshold_upper_bound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1876,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  17   49  213 1022 2249  939  177   28   14    3]\n",
      "[-0.03304  -0.025619 -0.018198 -0.010777 -0.003356  0.004065  0.011486\n",
      "  0.018907  0.026328  0.033749  0.04117 ]\n",
      "[-0.029329500000000175, -0.021908500000000164, -0.014487500000000153, -0.007066500000000142, 0.0003544999999998688, 0.00777549999999988, 0.01519649999999989, 0.0226174999999999, 0.030038499999999912, 0.03745949999999992]\n"
     ]
    }
   ],
   "source": [
    "bin_counts, bin_edges = np.histogram(price_changes, bins=number_of_bins)\n",
    "print(bin_counts)\n",
    "print(bin_edges)\n",
    "bin_max_diff = [(bin_edges[i] + bin_edges[i+1]) / 2 for i in range(len(bin_counts))]\n",
    "print(bin_max_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1877,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold upper bound: 0.00777549999999988\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def histogram_analysis(close_diff, number_of_bins):\n",
    "    # Perform histogram analysis\n",
    "    bin_counts, bin_edges = np.histogram(close_diff, bins=number_of_bins)\n",
    "    bin_max_diff = [(bin_edges[i] + bin_edges[i+1]) / 2 for i in range(len(bin_counts))]\n",
    "\n",
    "    # Sort bin_counts and bin_max_diff in descending order\n",
    "    sorted_indices = np.argsort(bin_counts)[::-1]\n",
    "    bin_counts = bin_counts[sorted_indices]\n",
    "    bin_max_diff = [bin_max_diff[i] for i in sorted_indices]\n",
    "\n",
    "    # Calculate sum of bin_counts\n",
    "    sum_bin_counts = np.sum(bin_counts)\n",
    "\n",
    "    # Find the threshold upper bound\n",
    "    temp_sum = 0\n",
    "    for i in range(number_of_bins):\n",
    "        temp_sum += bin_counts[i]\n",
    "        if temp_sum / sum_bin_counts > 0.85:\n",
    "            break\n",
    "\n",
    "    threshold_upper_bound = bin_max_diff[i]\n",
    "\n",
    "    return threshold_upper_bound\n",
    "\n",
    "# Example usage\n",
    "# close_diff = np.random.uniform(low=-0.01, high=0.01, size=1000)  # Example differences data\n",
    "number_of_bins = 10  # Example number of bins\n",
    "threshold_upper_bound = histogram_analysis(price_changes, number_of_bins)\n",
    "print(\"Threshold upper bound:\", threshold_upper_bound)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1879,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final threshold: 0.0024400000000000055\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def calculate_threshold(close_diff):\n",
    "    # Get the upper bound of the threshold value from histogram analysis\n",
    "    threshold_upper_bound = histogram_analysis(close_diff, number_of_bins)\n",
    "    \n",
    "    temp_threshold = 0\n",
    "    best_entropy = -float('inf')\n",
    "    threshold = None\n",
    "    \n",
    "    while temp_threshold < threshold_upper_bound:\n",
    "        labels = np.zeros(len(close_diff))  # Initialize labels with zeros\n",
    "        \n",
    "        # Assign labels based on threshold\n",
    "        indexes_incr = np.where(close_diff > temp_threshold)[0]\n",
    "        indexes_decr = np.where(-close_diff > temp_threshold)[0]\n",
    "        labels[indexes_incr] = 2\n",
    "        labels[indexes_decr] = 1\n",
    "        \n",
    "        # Calculate entropy\n",
    "        entropy = calculate_entropy(labels)\n",
    "        \n",
    "        # Update best_entropy and threshold if current entropy is better\n",
    "        if entropy > best_entropy:\n",
    "            best_entropy = entropy\n",
    "            threshold = temp_threshold\n",
    "        \n",
    "        # Increase temp_threshold for next iteration\n",
    "        temp_threshold += 0.00001\n",
    "    \n",
    "    return threshold\n",
    "\n",
    "# Function to calculate entropy\n",
    "def calculate_entropy(labels):\n",
    "    unique_labels, label_counts = np.unique(labels, return_counts=True)\n",
    "    probabilities = label_counts / len(labels)\n",
    "    entropy = -np.sum(probabilities * np.log2(probabilities))\n",
    "    return entropy\n",
    "\n",
    "# Example usage\n",
    "threshold = calculate_threshold(price_changes)\n",
    "print(\"Final threshold:\", threshold)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1880,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>c</th>\n",
       "      <th>MA_10</th>\n",
       "      <th>MACD_12_26</th>\n",
       "      <th>ROC_2</th>\n",
       "      <th>Momentum_4</th>\n",
       "      <th>RSI_10</th>\n",
       "      <th>BBL_20_2.0</th>\n",
       "      <th>BBM_20_2.0</th>\n",
       "      <th>BBU_20_2.0</th>\n",
       "      <th>CCI_20</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2006-02-07</th>\n",
       "      <td>1.19646</td>\n",
       "      <td>1.212396</td>\n",
       "      <td>-0.000074</td>\n",
       "      <td>-1.045406</td>\n",
       "      <td>-0.01834</td>\n",
       "      <td>36.217972</td>\n",
       "      <td>1.196244</td>\n",
       "      <td>1.212436</td>\n",
       "      <td>1.228628</td>\n",
       "      <td>-165.533230</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2006-02-08</th>\n",
       "      <td>1.19816</td>\n",
       "      <td>1.209542</td>\n",
       "      <td>-0.000946</td>\n",
       "      <td>-0.485050</td>\n",
       "      <td>-0.00964</td>\n",
       "      <td>38.532998</td>\n",
       "      <td>1.194933</td>\n",
       "      <td>1.212059</td>\n",
       "      <td>1.229185</td>\n",
       "      <td>-133.416803</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2006-02-09</th>\n",
       "      <td>1.19626</td>\n",
       "      <td>1.206658</td>\n",
       "      <td>-0.001769</td>\n",
       "      <td>-0.016716</td>\n",
       "      <td>-0.01284</td>\n",
       "      <td>36.871095</td>\n",
       "      <td>1.192758</td>\n",
       "      <td>1.211187</td>\n",
       "      <td>1.229616</td>\n",
       "      <td>-137.805786</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2006-02-10</th>\n",
       "      <td>1.19840</td>\n",
       "      <td>1.204328</td>\n",
       "      <td>-0.002223</td>\n",
       "      <td>0.020031</td>\n",
       "      <td>-0.00560</td>\n",
       "      <td>40.103967</td>\n",
       "      <td>1.191852</td>\n",
       "      <td>1.210892</td>\n",
       "      <td>1.229932</td>\n",
       "      <td>-105.776085</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2006-02-13</th>\n",
       "      <td>1.18960</td>\n",
       "      <td>1.202308</td>\n",
       "      <td>-0.003255</td>\n",
       "      <td>-0.556735</td>\n",
       "      <td>-0.00686</td>\n",
       "      <td>32.499577</td>\n",
       "      <td>1.188575</td>\n",
       "      <td>1.209562</td>\n",
       "      <td>1.230549</td>\n",
       "      <td>-167.999552</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-03-25</th>\n",
       "      <td>1.08030</td>\n",
       "      <td>1.089076</td>\n",
       "      <td>0.000777</td>\n",
       "      <td>-1.178214</td>\n",
       "      <td>-0.00694</td>\n",
       "      <td>38.811259</td>\n",
       "      <td>1.078868</td>\n",
       "      <td>1.087952</td>\n",
       "      <td>1.097036</td>\n",
       "      <td>-128.014550</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-03-26</th>\n",
       "      <td>1.08385</td>\n",
       "      <td>1.088139</td>\n",
       "      <td>0.000427</td>\n",
       "      <td>-0.253083</td>\n",
       "      <td>-0.00212</td>\n",
       "      <td>45.143745</td>\n",
       "      <td>1.078728</td>\n",
       "      <td>1.087897</td>\n",
       "      <td>1.097066</td>\n",
       "      <td>-70.215176</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-03-27</th>\n",
       "      <td>1.08302</td>\n",
       "      <td>1.087145</td>\n",
       "      <td>0.000082</td>\n",
       "      <td>0.251782</td>\n",
       "      <td>-0.01016</td>\n",
       "      <td>43.961831</td>\n",
       "      <td>1.078553</td>\n",
       "      <td>1.087835</td>\n",
       "      <td>1.097117</td>\n",
       "      <td>-80.347630</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-03-28</th>\n",
       "      <td>1.08138</td>\n",
       "      <td>1.085791</td>\n",
       "      <td>-0.000320</td>\n",
       "      <td>-0.227891</td>\n",
       "      <td>-0.00522</td>\n",
       "      <td>41.572286</td>\n",
       "      <td>1.078171</td>\n",
       "      <td>1.087716</td>\n",
       "      <td>1.097261</td>\n",
       "      <td>-105.605063</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-03-29</th>\n",
       "      <td>1.07866</td>\n",
       "      <td>1.084887</td>\n",
       "      <td>-0.000848</td>\n",
       "      <td>-0.402578</td>\n",
       "      <td>-0.00164</td>\n",
       "      <td>37.787271</td>\n",
       "      <td>1.077720</td>\n",
       "      <td>1.087610</td>\n",
       "      <td>1.097501</td>\n",
       "      <td>-140.927230</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4712 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  c     MA_10  MACD_12_26     ROC_2  Momentum_4     RSI_10  \\\n",
       "time                                                                         \n",
       "2006-02-07  1.19646  1.212396   -0.000074 -1.045406    -0.01834  36.217972   \n",
       "2006-02-08  1.19816  1.209542   -0.000946 -0.485050    -0.00964  38.532998   \n",
       "2006-02-09  1.19626  1.206658   -0.001769 -0.016716    -0.01284  36.871095   \n",
       "2006-02-10  1.19840  1.204328   -0.002223  0.020031    -0.00560  40.103967   \n",
       "2006-02-13  1.18960  1.202308   -0.003255 -0.556735    -0.00686  32.499577   \n",
       "...             ...       ...         ...       ...         ...        ...   \n",
       "2024-03-25  1.08030  1.089076    0.000777 -1.178214    -0.00694  38.811259   \n",
       "2024-03-26  1.08385  1.088139    0.000427 -0.253083    -0.00212  45.143745   \n",
       "2024-03-27  1.08302  1.087145    0.000082  0.251782    -0.01016  43.961831   \n",
       "2024-03-28  1.08138  1.085791   -0.000320 -0.227891    -0.00522  41.572286   \n",
       "2024-03-29  1.07866  1.084887   -0.000848 -0.402578    -0.00164  37.787271   \n",
       "\n",
       "            BBL_20_2.0  BBM_20_2.0  BBU_20_2.0      CCI_20  target  \n",
       "time                                                                \n",
       "2006-02-07    1.196244    1.212436    1.228628 -165.533230     0.0  \n",
       "2006-02-08    1.194933    1.212059    1.229185 -133.416803     0.0  \n",
       "2006-02-09    1.192758    1.211187    1.229616 -137.805786     0.0  \n",
       "2006-02-10    1.191852    1.210892    1.229932 -105.776085     1.0  \n",
       "2006-02-13    1.188575    1.209562    1.230549 -167.999552     0.0  \n",
       "...                ...         ...         ...         ...     ...  \n",
       "2024-03-25    1.078868    1.087952    1.097036 -128.014550     2.0  \n",
       "2024-03-26    1.078728    1.087897    1.097066  -70.215176     0.0  \n",
       "2024-03-27    1.078553    1.087835    1.097117  -80.347630     0.0  \n",
       "2024-03-28    1.078171    1.087716    1.097261 -105.605063     1.0  \n",
       "2024-03-29    1.077720    1.087610    1.097501 -140.927230     0.0  \n",
       "\n",
       "[4712 rows x 11 columns]"
      ]
     },
     "execution_count": 1880,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_tech = data.copy()\n",
    "data_tech['MA_10'] = ta.sma(data_tech['c'], length=10)\n",
    "data_tech['MACD_12_26'] = ta.macd(data_tech['c'], fast=12, slow=26).iloc[:, 0]\n",
    "data_tech['ROC_2'] = ta.roc(data_tech['c'], length=2)\n",
    "data_tech['Momentum_4'] = ta.mom(data_tech['c'], length=4)\n",
    "data_tech['RSI_10'] = ta.rsi(data_tech['c'], length=10)\n",
    "bb_data = ta.bbands(data_tech['c'], length=20)\n",
    "data_tech = pd.concat([data_tech, bb_data], axis=1)\n",
    "data_tech['CCI_20'] = ta.cci(data_tech['h'], data_tech['l'], data_tech['c'], length=20)\n",
    "data_tech = data_tech.iloc[26:]\n",
    "data_tech['price_diff'] = data_tech['c'].diff()\n",
    "data_tech['price_diff'] = data_tech['price_diff'].bfill()\n",
    "data_tech['target'] = 0\n",
    "data_tech.loc[data_tech['price_diff'] > threshold, 'target'] = 2\n",
    "data_tech.loc[data_tech['price_diff'] < -threshold, 'target'] = 1\n",
    "data_tech['target'] = data_tech['target'].shift(-day_ahead)\n",
    "data_tech['target'].fillna(0, inplace=True)\n",
    "data_tech.set_index('time', inplace=True)\n",
    "data_tech.drop(['h', 'l', 'price_diff', 'BBB_20_2.0', 'BBP_20_2.0'], axis=1, inplace=True)\n",
    "# print(data_tech.isna().sum())\n",
    "data_tech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1881,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_332141/2160447692.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_eco['price_diff'] = data_eco['c'].diff()\n",
      "/tmp/ipykernel_332141/2160447692.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_eco['price_diff'] = data_eco['price_diff'].bfill()\n",
      "/tmp/ipykernel_332141/2160447692.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_eco['target'] = 0\n",
      "/tmp/ipykernel_332141/2160447692.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_eco['target'] = data_eco['target'].shift(-day_ahead)\n",
      "/tmp/ipykernel_332141/2160447692.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_eco['target'].fillna(0, inplace=True)\n",
      "/tmp/ipykernel_332141/2160447692.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_eco.drop(['year_month', 'price_diff'], axis=1, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "data_eco['price_diff'] = data_eco['c'].diff()\n",
    "data_eco['price_diff'] = data_eco['price_diff'].bfill()\n",
    "data_eco['target'] = 0\n",
    "data_eco.loc[data_eco['price_diff'] > threshold, 'target'] = 2\n",
    "data_eco.loc[data_eco['price_diff'] < -threshold, 'target'] = 1\n",
    "data_eco['target'] = data_eco['target'].shift(-day_ahead)\n",
    "data_eco['target'].fillna(0, inplace=True)\n",
    "data_eco.set_index('time', inplace=True)\n",
    "data_eco.drop(['year_month', 'price_diff'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1882,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def data_normalized(X, y, test_size=0.2):\n",
    "    # Split the dataset into training and testing sets with no shuffling\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, shuffle=False)\n",
    "\n",
    "    # Initialize the MinMaxScaler\n",
    "    scaler = MinMaxScaler()\n",
    "\n",
    "    # Fit the scaler to the training data and transform it\n",
    "    X_train_normalized = scaler.fit_transform(X_train)\n",
    "\n",
    "    # Transform the testing data using the scaler fitted on the training data\n",
    "    X_test_normalized = scaler.transform(X_test)\n",
    "\n",
    "    return X_train_normalized, X_test_normalized, y_train, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1883,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.metrics import Metric\n",
    "import keras.backend as K\n",
    "\n",
    "class ProfitAccuracy(Metric):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(ProfitAccuracy, self).__init__(**kwargs)\n",
    "        self.true_dec = self.add_weight(name='true_dec', initializer='zeros')\n",
    "        self.true_inc = self.add_weight(name='true_inc', initializer='zeros')\n",
    "        self.false_dec_noact = self.add_weight(name='false_dec_noact', initializer='zeros')\n",
    "        self.false_inc_noact = self.add_weight(name='false_inc_noact', initializer='zeros')\n",
    "        self.false_inc_dec = self.add_weight(name='false_inc_dec', initializer='zeros')\n",
    "        self.false_dec_inc = self.add_weight(name='false_dec_inc', initializer='zeros')\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        # Convert y_true and y_pred to boolean arrays\n",
    "        y_true = K.argmax(y_true, axis=-1)\n",
    "        y_pred = K.argmax(y_pred, axis=-1)\n",
    "        \n",
    "        true_dec = K.sum(K.cast(K.equal(y_true, 1) & K.equal(y_pred, 1), 'float32'))\n",
    "        true_inc = K.sum(K.cast(K.equal(y_true, 2) & K.equal(y_pred, 2), 'float32'))\n",
    "        false_dec_noact = K.sum(K.cast(K.equal(y_true, 0) & K.equal(y_pred, 1), 'float32'))\n",
    "        false_inc_noact = K.sum(K.cast(K.equal(y_true, 0) & K.equal(y_pred, 2), 'float32'))\n",
    "        false_inc_dec = K.sum(K.cast(K.equal(y_true, 1) & K.equal(y_pred, 2), 'float32'))\n",
    "        false_dec_inc = K.sum(K.cast(K.equal(y_true, 2) & K.equal(y_pred, 1), 'float32'))\n",
    "\n",
    "\n",
    "        self.true_dec.assign_add(true_dec)\n",
    "        self.true_inc.assign_add(true_inc)\n",
    "        self.false_dec_noact.assign_add(false_dec_noact)\n",
    "        self.false_inc_noact.assign_add(false_inc_noact)\n",
    "        self.false_inc_dec.assign_add(false_inc_dec)\n",
    "        self.false_dec_inc.assign_add(false_dec_inc)\n",
    "\n",
    "    def result(self):\n",
    "        numerator = self.true_dec + self.true_inc\n",
    "        denominator = (\n",
    "            self.false_dec_noact + self.false_inc_noact +\n",
    "            self.true_dec + self.false_inc_dec +\n",
    "            self.false_dec_inc + self.true_inc\n",
    "        )\n",
    "\n",
    "        # Check for division by zero\n",
    "        if denominator == 0:\n",
    "            return 0.0\n",
    "        else:\n",
    "            return numerator / denominator\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.true_dec.assign(0)\n",
    "        self.true_inc.assign(0)\n",
    "        self.false_dec_noact.assign(0)\n",
    "        self.false_inc_noact.assign(0)\n",
    "        self.false_inc_dec.assign(0)\n",
    "        self.false_dec_inc.assign(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1884,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1885,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "time\n",
       "2006-02-07    0.0\n",
       "2006-02-08    0.0\n",
       "2006-02-09    0.0\n",
       "2006-02-10    1.0\n",
       "2006-02-13    0.0\n",
       "             ... \n",
       "2020-08-05    2.0\n",
       "2020-08-06    0.0\n",
       "2020-08-07    1.0\n",
       "2020-08-10    1.0\n",
       "2020-08-11    0.0\n",
       "Name: target, Length: 3769, dtype: float64"
      ]
     },
     "execution_count": 1885,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_eco = data_eco.drop(columns=['target'])\n",
    "y_eco = data_eco['target']\n",
    "\n",
    "X_train_normalized_eco, X_test_normalized_eco, y_train, y_test = data_normalized(X_eco, y_eco)\n",
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1886,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import to_categorical\n",
    "# from keras.layers import Dropout\n",
    "\n",
    "def train_lstm_model(X_train, X_test, y_train, y_test, lstm_units=lstm_units, epochs=epochs, batch_size=batch_size):\n",
    "    # Convert features to float32\n",
    "    X_train = X_train.astype('float32')\n",
    "    X_test = X_test.astype('float32')\n",
    "\n",
    "    # Convert labels to one-hot encoding\n",
    "    encoder = LabelEncoder()\n",
    "    encoder.fit(y_train)\n",
    "    encoded_y_train = encoder.transform(y_train)\n",
    "    encoded_y_test = encoder.transform(y_test)\n",
    "    y_train_onehot = to_categorical(encoded_y_train)\n",
    "    y_test_onehot = to_categorical(encoded_y_test)\n",
    "\n",
    "    # Reshape input data for LSTM\n",
    "    X_train = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
    "    X_test = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))\n",
    "\n",
    "    # Define the LSTM model\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(lstm_units, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=[ProfitAccuracy()])\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train_onehot, epochs=epochs, batch_size=batch_size, validation_data=(X_test, y_test_onehot))\n",
    "    \n",
    "    # Evaluate the model on the test set using custom metric\n",
    "    y_pred = model.predict(X_test)\n",
    "    profit_accuracy = ProfitAccuracy()(y_test_onehot, y_pred)\n",
    "    print(f\"Profit_accuracy = {profit_accuracy}\")\n",
    "\n",
    "    return y_pred, profit_accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1887,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 3s 9ms/step - loss: 1.0890 - profit_accuracy_110: 0.3520 - val_loss: 1.0817 - val_profit_accuracy_110: 0.0000e+00\n",
      "Epoch 2/100\n",
      "118/118 [==============================] - 1s 5ms/step - loss: 1.0812 - profit_accuracy_110: 0.3752 - val_loss: 1.0888 - val_profit_accuracy_110: 0.0000e+00\n",
      "Epoch 3/100\n",
      "118/118 [==============================] - 1s 5ms/step - loss: 1.0799 - profit_accuracy_110: 0.3759 - val_loss: 1.1257 - val_profit_accuracy_110: 0.0000e+00\n",
      "Epoch 4/100\n",
      "118/118 [==============================] - 1s 5ms/step - loss: 1.0780 - profit_accuracy_110: 0.3813 - val_loss: 1.1015 - val_profit_accuracy_110: 0.0000e+00\n",
      "Epoch 5/100\n",
      "118/118 [==============================] - 1s 5ms/step - loss: 1.0787 - profit_accuracy_110: 0.3769 - val_loss: 1.0973 - val_profit_accuracy_110: 0.0000e+00\n",
      "Epoch 6/100\n",
      "118/118 [==============================] - 1s 6ms/step - loss: 1.0777 - profit_accuracy_110: 0.3883 - val_loss: 1.1247 - val_profit_accuracy_110: 0.0000e+00\n",
      "Epoch 7/100\n",
      "118/118 [==============================] - 1s 6ms/step - loss: 1.0774 - profit_accuracy_110: 0.3904 - val_loss: 1.1164 - val_profit_accuracy_110: 0.0000e+00\n",
      "Epoch 8/100\n",
      "118/118 [==============================] - 1s 5ms/step - loss: 1.0776 - profit_accuracy_110: 0.3845 - val_loss: 1.1101 - val_profit_accuracy_110: 0.0000e+00\n",
      "Epoch 9/100\n",
      "118/118 [==============================] - 1s 6ms/step - loss: 1.0767 - profit_accuracy_110: 0.3822 - val_loss: 1.1501 - val_profit_accuracy_110: 0.0000e+00\n",
      "Epoch 10/100\n",
      "118/118 [==============================] - 1s 8ms/step - loss: 1.0760 - profit_accuracy_110: 0.3957 - val_loss: 1.0966 - val_profit_accuracy_110: 0.0000e+00\n",
      "Epoch 11/100\n",
      "118/118 [==============================] - 1s 9ms/step - loss: 1.0757 - profit_accuracy_110: 0.3882 - val_loss: 1.1262 - val_profit_accuracy_110: 0.0000e+00\n",
      "Epoch 12/100\n",
      "118/118 [==============================] - 1s 9ms/step - loss: 1.0756 - profit_accuracy_110: 0.4002 - val_loss: 1.1451 - val_profit_accuracy_110: 0.0000e+00\n",
      "Epoch 13/100\n",
      "118/118 [==============================] - 1s 8ms/step - loss: 1.0751 - profit_accuracy_110: 0.3977 - val_loss: 1.1026 - val_profit_accuracy_110: 0.0000e+00\n",
      "Epoch 14/100\n",
      "118/118 [==============================] - 1s 9ms/step - loss: 1.0748 - profit_accuracy_110: 0.4016 - val_loss: 1.1445 - val_profit_accuracy_110: 0.0000e+00\n",
      "Epoch 15/100\n",
      "118/118 [==============================] - 1s 7ms/step - loss: 1.0745 - profit_accuracy_110: 0.3910 - val_loss: 1.1340 - val_profit_accuracy_110: 0.0000e+00\n",
      "Epoch 16/100\n",
      "118/118 [==============================] - 1s 7ms/step - loss: 1.0750 - profit_accuracy_110: 0.3884 - val_loss: 1.1238 - val_profit_accuracy_110: 0.0000e+00\n",
      "Epoch 17/100\n",
      "118/118 [==============================] - 1s 8ms/step - loss: 1.0741 - profit_accuracy_110: 0.3920 - val_loss: 1.1256 - val_profit_accuracy_110: 0.0000e+00\n",
      "Epoch 18/100\n",
      "118/118 [==============================] - 1s 8ms/step - loss: 1.0746 - profit_accuracy_110: 0.3802 - val_loss: 1.1294 - val_profit_accuracy_110: 0.0000e+00\n",
      "Epoch 19/100\n",
      "118/118 [==============================] - 1s 8ms/step - loss: 1.0738 - profit_accuracy_110: 0.3963 - val_loss: 1.1592 - val_profit_accuracy_110: 0.0000e+00\n",
      "Epoch 20/100\n",
      "118/118 [==============================] - 1s 7ms/step - loss: 1.0737 - profit_accuracy_110: 0.3966 - val_loss: 1.1247 - val_profit_accuracy_110: 0.0000e+00\n",
      "Epoch 21/100\n",
      "118/118 [==============================] - 1s 7ms/step - loss: 1.0744 - profit_accuracy_110: 0.3884 - val_loss: 1.1474 - val_profit_accuracy_110: 0.0000e+00\n",
      "Epoch 22/100\n",
      "118/118 [==============================] - 1s 9ms/step - loss: 1.0736 - profit_accuracy_110: 0.3926 - val_loss: 1.2181 - val_profit_accuracy_110: 0.0000e+00\n",
      "Epoch 23/100\n",
      "118/118 [==============================] - 1s 8ms/step - loss: 1.0739 - profit_accuracy_110: 0.3798 - val_loss: 1.1316 - val_profit_accuracy_110: 0.0000e+00\n",
      "Epoch 24/100\n",
      "118/118 [==============================] - 1s 7ms/step - loss: 1.0730 - profit_accuracy_110: 0.3897 - val_loss: 1.1587 - val_profit_accuracy_110: 0.0000e+00\n",
      "Epoch 25/100\n",
      "118/118 [==============================] - 1s 7ms/step - loss: 1.0728 - profit_accuracy_110: 0.4065 - val_loss: 1.1593 - val_profit_accuracy_110: 0.0000e+00\n",
      "Epoch 26/100\n",
      "118/118 [==============================] - 1s 7ms/step - loss: 1.0728 - profit_accuracy_110: 0.3938 - val_loss: 1.1382 - val_profit_accuracy_110: 0.2500\n",
      "Epoch 27/100\n",
      "118/118 [==============================] - 1s 7ms/step - loss: 1.0733 - profit_accuracy_110: 0.3985 - val_loss: 1.1519 - val_profit_accuracy_110: 0.2500\n",
      "Epoch 28/100\n",
      "118/118 [==============================] - 1s 9ms/step - loss: 1.0728 - profit_accuracy_110: 0.3873 - val_loss: 1.1900 - val_profit_accuracy_110: 0.0000e+00\n",
      "Epoch 29/100\n",
      "118/118 [==============================] - 1s 7ms/step - loss: 1.0727 - profit_accuracy_110: 0.3815 - val_loss: 1.1129 - val_profit_accuracy_110: 0.3600\n",
      "Epoch 30/100\n",
      "118/118 [==============================] - 1s 7ms/step - loss: 1.0725 - profit_accuracy_110: 0.3892 - val_loss: 1.1756 - val_profit_accuracy_110: 0.0000e+00\n",
      "Epoch 31/100\n",
      "118/118 [==============================] - 1s 11ms/step - loss: 1.0722 - profit_accuracy_110: 0.3909 - val_loss: 1.1481 - val_profit_accuracy_110: 0.3000\n",
      "Epoch 32/100\n",
      "118/118 [==============================] - 1s 8ms/step - loss: 1.0725 - profit_accuracy_110: 0.3918 - val_loss: 1.1930 - val_profit_accuracy_110: 0.0000e+00\n",
      "Epoch 33/100\n",
      "118/118 [==============================] - 1s 6ms/step - loss: 1.0719 - profit_accuracy_110: 0.3882 - val_loss: 1.1579 - val_profit_accuracy_110: 0.2222\n",
      "Epoch 34/100\n",
      "118/118 [==============================] - 1s 6ms/step - loss: 1.0720 - profit_accuracy_110: 0.3975 - val_loss: 1.1215 - val_profit_accuracy_110: 0.3750\n",
      "Epoch 35/100\n",
      "118/118 [==============================] - 1s 5ms/step - loss: 1.0721 - profit_accuracy_110: 0.3873 - val_loss: 1.1595 - val_profit_accuracy_110: 0.2500\n",
      "Epoch 36/100\n",
      "118/118 [==============================] - 1s 5ms/step - loss: 1.0720 - profit_accuracy_110: 0.4029 - val_loss: 1.1389 - val_profit_accuracy_110: 0.3478\n",
      "Epoch 37/100\n",
      "118/118 [==============================] - 1s 7ms/step - loss: 1.0727 - profit_accuracy_110: 0.3837 - val_loss: 1.1498 - val_profit_accuracy_110: 0.3125\n",
      "Epoch 38/100\n",
      "118/118 [==============================] - 1s 7ms/step - loss: 1.0717 - profit_accuracy_110: 0.4022 - val_loss: 1.2148 - val_profit_accuracy_110: 0.0000e+00\n",
      "Epoch 39/100\n",
      "118/118 [==============================] - 1s 10ms/step - loss: 1.0719 - profit_accuracy_110: 0.3912 - val_loss: 1.1724 - val_profit_accuracy_110: 0.3000\n",
      "Epoch 40/100\n",
      "118/118 [==============================] - 1s 8ms/step - loss: 1.0727 - profit_accuracy_110: 0.3901 - val_loss: 1.2098 - val_profit_accuracy_110: 0.0000e+00\n",
      "Epoch 41/100\n",
      "118/118 [==============================] - 1s 7ms/step - loss: 1.0713 - profit_accuracy_110: 0.3919 - val_loss: 1.1882 - val_profit_accuracy_110: 0.3077\n",
      "Epoch 42/100\n",
      "118/118 [==============================] - 1s 6ms/step - loss: 1.0713 - profit_accuracy_110: 0.3904 - val_loss: 1.1707 - val_profit_accuracy_110: 0.2222\n",
      "Epoch 43/100\n",
      "118/118 [==============================] - 1s 6ms/step - loss: 1.0714 - profit_accuracy_110: 0.3934 - val_loss: 1.1767 - val_profit_accuracy_110: 0.2222\n",
      "Epoch 44/100\n",
      "118/118 [==============================] - 1s 6ms/step - loss: 1.0712 - profit_accuracy_110: 0.3934 - val_loss: 1.1850 - val_profit_accuracy_110: 0.0000e+00\n",
      "Epoch 45/100\n",
      "118/118 [==============================] - 1s 7ms/step - loss: 1.0711 - profit_accuracy_110: 0.3900 - val_loss: 1.1429 - val_profit_accuracy_110: 0.4048\n",
      "Epoch 46/100\n",
      "118/118 [==============================] - 1s 7ms/step - loss: 1.0715 - profit_accuracy_110: 0.3938 - val_loss: 1.1892 - val_profit_accuracy_110: 0.3333\n",
      "Epoch 47/100\n",
      "118/118 [==============================] - 1s 6ms/step - loss: 1.0713 - profit_accuracy_110: 0.3894 - val_loss: 1.1938 - val_profit_accuracy_110: 0.0000e+00\n",
      "Epoch 48/100\n",
      "118/118 [==============================] - 1s 6ms/step - loss: 1.0709 - profit_accuracy_110: 0.3939 - val_loss: 1.1909 - val_profit_accuracy_110: 0.0000e+00\n",
      "Epoch 49/100\n",
      "118/118 [==============================] - 1s 5ms/step - loss: 1.0712 - profit_accuracy_110: 0.3955 - val_loss: 1.1960 - val_profit_accuracy_110: 0.0000e+00\n",
      "Epoch 50/100\n",
      "118/118 [==============================] - 1s 6ms/step - loss: 1.0710 - profit_accuracy_110: 0.3977 - val_loss: 1.2093 - val_profit_accuracy_110: 0.0000e+00\n",
      "Epoch 51/100\n",
      "118/118 [==============================] - 1s 6ms/step - loss: 1.0711 - profit_accuracy_110: 0.3942 - val_loss: 1.2239 - val_profit_accuracy_110: 0.0000e+00\n",
      "Epoch 52/100\n",
      "118/118 [==============================] - 1s 5ms/step - loss: 1.0715 - profit_accuracy_110: 0.3998 - val_loss: 1.1857 - val_profit_accuracy_110: 0.2857\n",
      "Epoch 53/100\n",
      "118/118 [==============================] - 1s 7ms/step - loss: 1.0713 - profit_accuracy_110: 0.3887 - val_loss: 1.2109 - val_profit_accuracy_110: 0.0000e+00\n",
      "Epoch 54/100\n",
      "118/118 [==============================] - 1s 6ms/step - loss: 1.0712 - profit_accuracy_110: 0.3964 - val_loss: 1.2159 - val_profit_accuracy_110: 0.0000e+00\n",
      "Epoch 55/100\n",
      "118/118 [==============================] - 1s 4ms/step - loss: 1.0711 - profit_accuracy_110: 0.3921 - val_loss: 1.1737 - val_profit_accuracy_110: 0.2500\n",
      "Epoch 56/100\n",
      "118/118 [==============================] - 1s 5ms/step - loss: 1.0705 - profit_accuracy_110: 0.3990 - val_loss: 1.1468 - val_profit_accuracy_110: 0.2903\n",
      "Epoch 57/100\n",
      "118/118 [==============================] - 1s 5ms/step - loss: 1.0702 - profit_accuracy_110: 0.3824 - val_loss: 1.1821 - val_profit_accuracy_110: 0.2857\n",
      "Epoch 58/100\n",
      "118/118 [==============================] - 1s 5ms/step - loss: 1.0702 - profit_accuracy_110: 0.3932 - val_loss: 1.1608 - val_profit_accuracy_110: 0.3125\n",
      "Epoch 59/100\n",
      "118/118 [==============================] - 1s 5ms/step - loss: 1.0706 - profit_accuracy_110: 0.3976 - val_loss: 1.2448 - val_profit_accuracy_110: 0.0000e+00\n",
      "Epoch 60/100\n",
      "118/118 [==============================] - 1s 5ms/step - loss: 1.0713 - profit_accuracy_110: 0.3992 - val_loss: 1.1846 - val_profit_accuracy_110: 0.3125\n",
      "Epoch 61/100\n",
      "118/118 [==============================] - 1s 5ms/step - loss: 1.0701 - profit_accuracy_110: 0.4003 - val_loss: 1.1863 - val_profit_accuracy_110: 0.3125\n",
      "Epoch 62/100\n",
      "118/118 [==============================] - 1s 5ms/step - loss: 1.0697 - profit_accuracy_110: 0.3985 - val_loss: 1.1982 - val_profit_accuracy_110: 0.2500\n",
      "Epoch 63/100\n",
      "118/118 [==============================] - 1s 6ms/step - loss: 1.0699 - profit_accuracy_110: 0.3936 - val_loss: 1.1909 - val_profit_accuracy_110: 0.2222\n",
      "Epoch 64/100\n",
      "118/118 [==============================] - 1s 5ms/step - loss: 1.0705 - profit_accuracy_110: 0.3942 - val_loss: 1.2184 - val_profit_accuracy_110: 0.2500\n",
      "Epoch 65/100\n",
      "118/118 [==============================] - 1s 5ms/step - loss: 1.0706 - profit_accuracy_110: 0.3962 - val_loss: 1.2169 - val_profit_accuracy_110: 0.3000\n",
      "Epoch 66/100\n",
      "118/118 [==============================] - 1s 5ms/step - loss: 1.0703 - profit_accuracy_110: 0.3987 - val_loss: 1.1667 - val_profit_accuracy_110: 0.3500\n",
      "Epoch 67/100\n",
      "118/118 [==============================] - 1s 5ms/step - loss: 1.0702 - profit_accuracy_110: 0.3902 - val_loss: 1.1744 - val_profit_accuracy_110: 0.3333\n",
      "Epoch 68/100\n",
      "118/118 [==============================] - 1s 5ms/step - loss: 1.0694 - profit_accuracy_110: 0.3945 - val_loss: 1.2524 - val_profit_accuracy_110: 0.0000e+00\n",
      "Epoch 69/100\n",
      "118/118 [==============================] - 1s 7ms/step - loss: 1.0699 - profit_accuracy_110: 0.3918 - val_loss: 1.1552 - val_profit_accuracy_110: 0.3158\n",
      "Epoch 70/100\n",
      "118/118 [==============================] - 1s 8ms/step - loss: 1.0696 - profit_accuracy_110: 0.4005 - val_loss: 1.1809 - val_profit_accuracy_110: 0.3158\n",
      "Epoch 71/100\n",
      "118/118 [==============================] - 1s 7ms/step - loss: 1.0701 - profit_accuracy_110: 0.3972 - val_loss: 1.1712 - val_profit_accuracy_110: 0.3636\n",
      "Epoch 72/100\n",
      "118/118 [==============================] - 1s 6ms/step - loss: 1.0700 - profit_accuracy_110: 0.4005 - val_loss: 1.1522 - val_profit_accuracy_110: 0.3214\n",
      "Epoch 73/100\n",
      "118/118 [==============================] - 1s 7ms/step - loss: 1.0697 - profit_accuracy_110: 0.4042 - val_loss: 1.2245 - val_profit_accuracy_110: 0.0000e+00\n",
      "Epoch 74/100\n",
      "118/118 [==============================] - 1s 6ms/step - loss: 1.0711 - profit_accuracy_110: 0.4020 - val_loss: 1.2305 - val_profit_accuracy_110: 0.0000e+00\n",
      "Epoch 75/100\n",
      "118/118 [==============================] - 1s 7ms/step - loss: 1.0702 - profit_accuracy_110: 0.3951 - val_loss: 1.1707 - val_profit_accuracy_110: 0.3462\n",
      "Epoch 76/100\n",
      "118/118 [==============================] - 1s 6ms/step - loss: 1.0698 - profit_accuracy_110: 0.3964 - val_loss: 1.1965 - val_profit_accuracy_110: 0.2857\n",
      "Epoch 77/100\n",
      "118/118 [==============================] - 1s 6ms/step - loss: 1.0697 - profit_accuracy_110: 0.4014 - val_loss: 1.2284 - val_profit_accuracy_110: 0.3333\n",
      "Epoch 78/100\n",
      "118/118 [==============================] - 1s 7ms/step - loss: 1.0696 - profit_accuracy_110: 0.3993 - val_loss: 1.1721 - val_profit_accuracy_110: 0.3500\n",
      "Epoch 79/100\n",
      "118/118 [==============================] - 1s 6ms/step - loss: 1.0699 - profit_accuracy_110: 0.3978 - val_loss: 1.1992 - val_profit_accuracy_110: 0.3077\n",
      "Epoch 80/100\n",
      "118/118 [==============================] - 1s 6ms/step - loss: 1.0688 - profit_accuracy_110: 0.3946 - val_loss: 1.2304 - val_profit_accuracy_110: 0.0000e+00\n",
      "Epoch 81/100\n",
      "118/118 [==============================] - 1s 5ms/step - loss: 1.0700 - profit_accuracy_110: 0.3977 - val_loss: 1.1718 - val_profit_accuracy_110: 0.3500\n",
      "Epoch 82/100\n",
      "118/118 [==============================] - 1s 5ms/step - loss: 1.0699 - profit_accuracy_110: 0.3965 - val_loss: 1.2074 - val_profit_accuracy_110: 0.3077\n",
      "Epoch 83/100\n",
      "118/118 [==============================] - 1s 4ms/step - loss: 1.0693 - profit_accuracy_110: 0.3974 - val_loss: 1.1862 - val_profit_accuracy_110: 0.3500\n",
      "Epoch 84/100\n",
      "118/118 [==============================] - 1s 5ms/step - loss: 1.0686 - profit_accuracy_110: 0.3917 - val_loss: 1.2604 - val_profit_accuracy_110: 0.0000e+00\n",
      "Epoch 85/100\n",
      "118/118 [==============================] - 1s 5ms/step - loss: 1.0701 - profit_accuracy_110: 0.3927 - val_loss: 1.2100 - val_profit_accuracy_110: 0.2222\n",
      "Epoch 86/100\n",
      "118/118 [==============================] - 1s 5ms/step - loss: 1.0691 - profit_accuracy_110: 0.3957 - val_loss: 1.2471 - val_profit_accuracy_110: 0.0000e+00\n",
      "Epoch 87/100\n",
      "118/118 [==============================] - 1s 5ms/step - loss: 1.0693 - profit_accuracy_110: 0.4038 - val_loss: 1.2134 - val_profit_accuracy_110: 0.3333\n",
      "Epoch 88/100\n",
      "118/118 [==============================] - 1s 5ms/step - loss: 1.0690 - profit_accuracy_110: 0.4000 - val_loss: 1.1935 - val_profit_accuracy_110: 0.3333\n",
      "Epoch 89/100\n",
      "118/118 [==============================] - 1s 5ms/step - loss: 1.0690 - profit_accuracy_110: 0.4075 - val_loss: 1.1705 - val_profit_accuracy_110: 0.3750\n",
      "Epoch 90/100\n",
      "118/118 [==============================] - 1s 5ms/step - loss: 1.0698 - profit_accuracy_110: 0.3922 - val_loss: 1.2037 - val_profit_accuracy_110: 0.2857\n",
      "Epoch 91/100\n",
      "118/118 [==============================] - 1s 6ms/step - loss: 1.0693 - profit_accuracy_110: 0.4060 - val_loss: 1.1998 - val_profit_accuracy_110: 0.3333\n",
      "Epoch 92/100\n",
      "118/118 [==============================] - 1s 6ms/step - loss: 1.0694 - profit_accuracy_110: 0.3980 - val_loss: 1.1969 - val_profit_accuracy_110: 0.3125\n",
      "Epoch 93/100\n",
      "118/118 [==============================] - 1s 5ms/step - loss: 1.0687 - profit_accuracy_110: 0.4066 - val_loss: 1.2005 - val_profit_accuracy_110: 0.0000e+00\n",
      "Epoch 94/100\n",
      "118/118 [==============================] - 1s 5ms/step - loss: 1.0699 - profit_accuracy_110: 0.3990 - val_loss: 1.2175 - val_profit_accuracy_110: 0.2500\n",
      "Epoch 95/100\n",
      "118/118 [==============================] - 1s 5ms/step - loss: 1.0693 - profit_accuracy_110: 0.4005 - val_loss: 1.1659 - val_profit_accuracy_110: 0.3000\n",
      "Epoch 96/100\n",
      "118/118 [==============================] - 1s 6ms/step - loss: 1.0695 - profit_accuracy_110: 0.4024 - val_loss: 1.1890 - val_profit_accuracy_110: 0.3333\n",
      "Epoch 97/100\n",
      "118/118 [==============================] - 1s 5ms/step - loss: 1.0689 - profit_accuracy_110: 0.4019 - val_loss: 1.1834 - val_profit_accuracy_110: 0.2857\n",
      "Epoch 98/100\n",
      "118/118 [==============================] - 1s 5ms/step - loss: 1.0684 - profit_accuracy_110: 0.4009 - val_loss: 1.1919 - val_profit_accuracy_110: 0.3636\n",
      "Epoch 99/100\n",
      "118/118 [==============================] - 1s 6ms/step - loss: 1.0692 - profit_accuracy_110: 0.4009 - val_loss: 1.2017 - val_profit_accuracy_110: 0.3571\n",
      "Epoch 100/100\n",
      "118/118 [==============================] - 1s 5ms/step - loss: 1.0696 - profit_accuracy_110: 0.4007 - val_loss: 1.2172 - val_profit_accuracy_110: 0.4000\n",
      "30/30 [==============================] - 1s 3ms/step\n",
      "Profit_accuracy = 0.4000000059604645\n"
     ]
    }
   ],
   "source": [
    "y_eco = data_eco['target']\n",
    "X_eco = data_eco.drop(columns=['target'])\n",
    "\n",
    "X_train_normalized_eco, X_test_normalized_eco, y_train, y_test = data_normalized(X_eco, y_eco)\n",
    "y_pred, profit_accuracy = train_lstm_model(X_train_normalized_eco, X_test_normalized_eco, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1888,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "118/118 [==============================] - 4s 9ms/step - loss: 1.0959 - profit_accuracy_112: 0.3431 - val_loss: 1.0979 - val_profit_accuracy_112: 0.2746\n",
      "Epoch 2/100\n",
      "118/118 [==============================] - 1s 6ms/step - loss: 1.0926 - profit_accuracy_112: 0.3665 - val_loss: 1.0977 - val_profit_accuracy_112: 0.2828\n",
      "Epoch 3/100\n",
      "118/118 [==============================] - 1s 7ms/step - loss: 1.0920 - profit_accuracy_112: 0.3591 - val_loss: 1.0916 - val_profit_accuracy_112: 0.0000e+00\n",
      "Epoch 4/100\n",
      "118/118 [==============================] - 1s 7ms/step - loss: 1.0913 - profit_accuracy_112: 0.3616 - val_loss: 1.0920 - val_profit_accuracy_112: 0.0000e+00\n",
      "Epoch 5/100\n",
      "118/118 [==============================] - 1s 6ms/step - loss: 1.0905 - profit_accuracy_112: 0.3613 - val_loss: 1.0941 - val_profit_accuracy_112: 0.2993\n",
      "Epoch 6/100\n",
      "118/118 [==============================] - 1s 6ms/step - loss: 1.0906 - profit_accuracy_112: 0.3721 - val_loss: 1.0905 - val_profit_accuracy_112: 0.0000e+00\n",
      "Epoch 7/100\n",
      "118/118 [==============================] - 1s 5ms/step - loss: 1.0905 - profit_accuracy_112: 0.3760 - val_loss: 1.0921 - val_profit_accuracy_112: 0.2800\n",
      "Epoch 8/100\n",
      "118/118 [==============================] - 1s 5ms/step - loss: 1.0901 - profit_accuracy_112: 0.3727 - val_loss: 1.0949 - val_profit_accuracy_112: 0.2816\n",
      "Epoch 9/100\n",
      "118/118 [==============================] - 1s 5ms/step - loss: 1.0897 - profit_accuracy_112: 0.3707 - val_loss: 1.0914 - val_profit_accuracy_112: 0.2667\n",
      "Epoch 10/100\n",
      "118/118 [==============================] - 1s 5ms/step - loss: 1.0905 - profit_accuracy_112: 0.3859 - val_loss: 1.0911 - val_profit_accuracy_112: 0.3913\n",
      "Epoch 11/100\n",
      "118/118 [==============================] - 1s 6ms/step - loss: 1.0888 - profit_accuracy_112: 0.3792 - val_loss: 1.0910 - val_profit_accuracy_112: 0.2000\n",
      "Epoch 12/100\n",
      "118/118 [==============================] - 1s 6ms/step - loss: 1.0892 - profit_accuracy_112: 0.3733 - val_loss: 1.0898 - val_profit_accuracy_112: 0.0000e+00\n",
      "Epoch 13/100\n",
      "118/118 [==============================] - 1s 5ms/step - loss: 1.0892 - profit_accuracy_112: 0.3714 - val_loss: 1.0909 - val_profit_accuracy_112: 0.3000\n",
      "Epoch 14/100\n",
      "118/118 [==============================] - 1s 6ms/step - loss: 1.0879 - profit_accuracy_112: 0.3801 - val_loss: 1.0928 - val_profit_accuracy_112: 0.2994\n",
      "Epoch 15/100\n",
      "118/118 [==============================] - 1s 7ms/step - loss: 1.0878 - profit_accuracy_112: 0.3838 - val_loss: 1.0889 - val_profit_accuracy_112: 0.0000e+00\n",
      "Epoch 16/100\n",
      "118/118 [==============================] - 1s 8ms/step - loss: 1.0887 - profit_accuracy_112: 0.3696 - val_loss: 1.0911 - val_profit_accuracy_112: 0.3289\n",
      "Epoch 17/100\n",
      "118/118 [==============================] - 1s 8ms/step - loss: 1.0881 - profit_accuracy_112: 0.3772 - val_loss: 1.0907 - val_profit_accuracy_112: 0.3182\n",
      "Epoch 18/100\n",
      "118/118 [==============================] - 1s 8ms/step - loss: 1.0880 - profit_accuracy_112: 0.3707 - val_loss: 1.0910 - val_profit_accuracy_112: 0.2564\n",
      "Epoch 19/100\n",
      "118/118 [==============================] - 1s 8ms/step - loss: 1.0875 - profit_accuracy_112: 0.3785 - val_loss: 1.0919 - val_profit_accuracy_112: 0.2479\n",
      "Epoch 20/100\n",
      "118/118 [==============================] - 1s 8ms/step - loss: 1.0878 - profit_accuracy_112: 0.3799 - val_loss: 1.0950 - val_profit_accuracy_112: 0.3018\n",
      "Epoch 21/100\n",
      "118/118 [==============================] - 1s 7ms/step - loss: 1.0870 - profit_accuracy_112: 0.3637 - val_loss: 1.0893 - val_profit_accuracy_112: 0.3051\n",
      "Epoch 22/100\n",
      "118/118 [==============================] - 1s 8ms/step - loss: 1.0876 - profit_accuracy_112: 0.3882 - val_loss: 1.0900 - val_profit_accuracy_112: 0.3425\n",
      "Epoch 23/100\n",
      "118/118 [==============================] - 1s 6ms/step - loss: 1.0869 - profit_accuracy_112: 0.3856 - val_loss: 1.0906 - val_profit_accuracy_112: 0.3333\n",
      "Epoch 24/100\n",
      "118/118 [==============================] - 1s 6ms/step - loss: 1.0866 - profit_accuracy_112: 0.3816 - val_loss: 1.0881 - val_profit_accuracy_112: 0.3500\n",
      "Epoch 25/100\n",
      "118/118 [==============================] - 1s 5ms/step - loss: 1.0864 - profit_accuracy_112: 0.3693 - val_loss: 1.0891 - val_profit_accuracy_112: 0.2857\n",
      "Epoch 26/100\n",
      "118/118 [==============================] - 1s 6ms/step - loss: 1.0859 - profit_accuracy_112: 0.3895 - val_loss: 1.0941 - val_profit_accuracy_112: 0.3142\n",
      "Epoch 27/100\n",
      "118/118 [==============================] - 1s 5ms/step - loss: 1.0856 - profit_accuracy_112: 0.3883 - val_loss: 1.0877 - val_profit_accuracy_112: 0.2500\n",
      "Epoch 28/100\n",
      "118/118 [==============================] - 1s 6ms/step - loss: 1.0853 - profit_accuracy_112: 0.3790 - val_loss: 1.0877 - val_profit_accuracy_112: 0.3636\n",
      "Epoch 29/100\n",
      "118/118 [==============================] - 1s 8ms/step - loss: 1.0856 - profit_accuracy_112: 0.3860 - val_loss: 1.0917 - val_profit_accuracy_112: 0.3398\n",
      "Epoch 30/100\n",
      "118/118 [==============================] - 1s 7ms/step - loss: 1.0847 - profit_accuracy_112: 0.3826 - val_loss: 1.0900 - val_profit_accuracy_112: 0.3667\n",
      "Epoch 31/100\n",
      "118/118 [==============================] - 1s 6ms/step - loss: 1.0853 - profit_accuracy_112: 0.3828 - val_loss: 1.0881 - val_profit_accuracy_112: 0.2683\n",
      "Epoch 32/100\n",
      "118/118 [==============================] - 1s 5ms/step - loss: 1.0843 - profit_accuracy_112: 0.3741 - val_loss: 1.0914 - val_profit_accuracy_112: 0.3399\n",
      "Epoch 33/100\n",
      "118/118 [==============================] - 1s 5ms/step - loss: 1.0846 - profit_accuracy_112: 0.3776 - val_loss: 1.0891 - val_profit_accuracy_112: 0.3609\n",
      "Epoch 34/100\n",
      "118/118 [==============================] - 1s 5ms/step - loss: 1.0845 - profit_accuracy_112: 0.3880 - val_loss: 1.0895 - val_profit_accuracy_112: 0.3899\n",
      "Epoch 35/100\n",
      "118/118 [==============================] - 1s 5ms/step - loss: 1.0842 - profit_accuracy_112: 0.3926 - val_loss: 1.0930 - val_profit_accuracy_112: 0.3503\n",
      "Epoch 36/100\n",
      "118/118 [==============================] - 1s 5ms/step - loss: 1.0840 - profit_accuracy_112: 0.3713 - val_loss: 1.0880 - val_profit_accuracy_112: 0.3793\n",
      "Epoch 37/100\n",
      "118/118 [==============================] - 1s 5ms/step - loss: 1.0844 - profit_accuracy_112: 0.3825 - val_loss: 1.0873 - val_profit_accuracy_112: 0.3854\n",
      "Epoch 38/100\n",
      "118/118 [==============================] - 1s 6ms/step - loss: 1.0826 - profit_accuracy_112: 0.3833 - val_loss: 1.0882 - val_profit_accuracy_112: 0.3401\n",
      "Epoch 39/100\n",
      "118/118 [==============================] - 1s 5ms/step - loss: 1.0826 - profit_accuracy_112: 0.3857 - val_loss: 1.0934 - val_profit_accuracy_112: 0.3393\n",
      "Epoch 40/100\n",
      "118/118 [==============================] - 1s 5ms/step - loss: 1.0833 - profit_accuracy_112: 0.3733 - val_loss: 1.0868 - val_profit_accuracy_112: 0.3617\n",
      "Epoch 41/100\n",
      "118/118 [==============================] - 1s 5ms/step - loss: 1.0831 - profit_accuracy_112: 0.3857 - val_loss: 1.0856 - val_profit_accuracy_112: 0.4247\n",
      "Epoch 42/100\n",
      "118/118 [==============================] - 1s 5ms/step - loss: 1.0838 - profit_accuracy_112: 0.3810 - val_loss: 1.0874 - val_profit_accuracy_112: 0.3333\n",
      "Epoch 43/100\n",
      "118/118 [==============================] - 1s 5ms/step - loss: 1.0834 - profit_accuracy_112: 0.3867 - val_loss: 1.0902 - val_profit_accuracy_112: 0.3574\n",
      "Epoch 44/100\n",
      "118/118 [==============================] - 1s 6ms/step - loss: 1.0824 - profit_accuracy_112: 0.3903 - val_loss: 1.0869 - val_profit_accuracy_112: 0.3500\n",
      "Epoch 45/100\n",
      "118/118 [==============================] - 1s 6ms/step - loss: 1.0824 - profit_accuracy_112: 0.3927 - val_loss: 1.0896 - val_profit_accuracy_112: 0.3800\n",
      "Epoch 46/100\n",
      "118/118 [==============================] - 1s 6ms/step - loss: 1.0828 - profit_accuracy_112: 0.3834 - val_loss: 1.0864 - val_profit_accuracy_112: 0.4000\n",
      "Epoch 47/100\n",
      "118/118 [==============================] - 1s 6ms/step - loss: 1.0816 - profit_accuracy_112: 0.3941 - val_loss: 1.0878 - val_profit_accuracy_112: 0.3667\n",
      "Epoch 48/100\n",
      "118/118 [==============================] - 1s 5ms/step - loss: 1.0806 - profit_accuracy_112: 0.3793 - val_loss: 1.0893 - val_profit_accuracy_112: 0.3534\n",
      "Epoch 49/100\n",
      "118/118 [==============================] - 1s 5ms/step - loss: 1.0806 - profit_accuracy_112: 0.3950 - val_loss: 1.0948 - val_profit_accuracy_112: 0.3351\n",
      "Epoch 50/100\n",
      "118/118 [==============================] - 1s 4ms/step - loss: 1.0828 - profit_accuracy_112: 0.3889 - val_loss: 1.0860 - val_profit_accuracy_112: 0.3500\n",
      "Epoch 51/100\n",
      "118/118 [==============================] - 1s 5ms/step - loss: 1.0816 - profit_accuracy_112: 0.3887 - val_loss: 1.0871 - val_profit_accuracy_112: 0.3648\n",
      "Epoch 52/100\n",
      "118/118 [==============================] - 1s 5ms/step - loss: 1.0816 - profit_accuracy_112: 0.3924 - val_loss: 1.0863 - val_profit_accuracy_112: 0.3588\n",
      "Epoch 53/100\n",
      "118/118 [==============================] - 1s 5ms/step - loss: 1.0808 - profit_accuracy_112: 0.3876 - val_loss: 1.0868 - val_profit_accuracy_112: 0.3585\n",
      "Epoch 54/100\n",
      "118/118 [==============================] - 1s 5ms/step - loss: 1.0806 - profit_accuracy_112: 0.3829 - val_loss: 1.0883 - val_profit_accuracy_112: 0.3650\n",
      "Epoch 55/100\n",
      "118/118 [==============================] - 1s 5ms/step - loss: 1.0803 - profit_accuracy_112: 0.3902 - val_loss: 1.0861 - val_profit_accuracy_112: 0.3740\n",
      "Epoch 56/100\n",
      "118/118 [==============================] - 1s 5ms/step - loss: 1.0806 - profit_accuracy_112: 0.3824 - val_loss: 1.0860 - val_profit_accuracy_112: 0.3826\n",
      "Epoch 57/100\n",
      "118/118 [==============================] - 1s 5ms/step - loss: 1.0810 - profit_accuracy_112: 0.4011 - val_loss: 1.0889 - val_profit_accuracy_112: 0.3762\n",
      "Epoch 58/100\n",
      "118/118 [==============================] - 1s 5ms/step - loss: 1.0812 - profit_accuracy_112: 0.3863 - val_loss: 1.0913 - val_profit_accuracy_112: 0.3464\n",
      "Epoch 59/100\n",
      "118/118 [==============================] - 1s 6ms/step - loss: 1.0807 - profit_accuracy_112: 0.3825 - val_loss: 1.0854 - val_profit_accuracy_112: 0.4167\n",
      "Epoch 60/100\n",
      "118/118 [==============================] - 1s 5ms/step - loss: 1.0803 - profit_accuracy_112: 0.3965 - val_loss: 1.0860 - val_profit_accuracy_112: 0.3661\n",
      "Epoch 61/100\n",
      "118/118 [==============================] - 1s 4ms/step - loss: 1.0817 - profit_accuracy_112: 0.3798 - val_loss: 1.0891 - val_profit_accuracy_112: 0.3867\n",
      "Epoch 62/100\n",
      "118/118 [==============================] - 1s 6ms/step - loss: 1.0794 - profit_accuracy_112: 0.3800 - val_loss: 1.0864 - val_profit_accuracy_112: 0.4000\n",
      "Epoch 63/100\n",
      "118/118 [==============================] - 1s 6ms/step - loss: 1.0794 - profit_accuracy_112: 0.3859 - val_loss: 1.0872 - val_profit_accuracy_112: 0.3786\n",
      "Epoch 64/100\n",
      "118/118 [==============================] - 1s 6ms/step - loss: 1.0813 - profit_accuracy_112: 0.3784 - val_loss: 1.0880 - val_profit_accuracy_112: 0.3636\n",
      "Epoch 65/100\n",
      "118/118 [==============================] - 1s 5ms/step - loss: 1.0800 - profit_accuracy_112: 0.3804 - val_loss: 1.0880 - val_profit_accuracy_112: 0.3548\n",
      "Epoch 66/100\n",
      "118/118 [==============================] - 1s 5ms/step - loss: 1.0807 - profit_accuracy_112: 0.3889 - val_loss: 1.0868 - val_profit_accuracy_112: 0.3836\n",
      "Epoch 67/100\n",
      "118/118 [==============================] - 1s 5ms/step - loss: 1.0802 - profit_accuracy_112: 0.3791 - val_loss: 1.0891 - val_profit_accuracy_112: 0.3729\n",
      "Epoch 68/100\n",
      "118/118 [==============================] - 1s 5ms/step - loss: 1.0786 - profit_accuracy_112: 0.3866 - val_loss: 1.0966 - val_profit_accuracy_112: 0.3194\n",
      "Epoch 69/100\n",
      "118/118 [==============================] - 1s 5ms/step - loss: 1.0786 - profit_accuracy_112: 0.3885 - val_loss: 1.0886 - val_profit_accuracy_112: 0.3828\n",
      "Epoch 70/100\n",
      "118/118 [==============================] - 1s 6ms/step - loss: 1.0790 - profit_accuracy_112: 0.3848 - val_loss: 1.0910 - val_profit_accuracy_112: 0.3532\n",
      "Epoch 71/100\n",
      "118/118 [==============================] - 1s 5ms/step - loss: 1.0801 - profit_accuracy_112: 0.3884 - val_loss: 1.0903 - val_profit_accuracy_112: 0.3454\n",
      "Epoch 72/100\n",
      "118/118 [==============================] - 1s 5ms/step - loss: 1.0792 - profit_accuracy_112: 0.3920 - val_loss: 1.0892 - val_profit_accuracy_112: 0.3820\n",
      "Epoch 73/100\n",
      "118/118 [==============================] - 1s 4ms/step - loss: 1.0779 - profit_accuracy_112: 0.3917 - val_loss: 1.0866 - val_profit_accuracy_112: 0.3788\n",
      "Epoch 74/100\n",
      "118/118 [==============================] - 1s 5ms/step - loss: 1.0793 - profit_accuracy_112: 0.3906 - val_loss: 1.0935 - val_profit_accuracy_112: 0.3449\n",
      "Epoch 75/100\n",
      "118/118 [==============================] - 1s 5ms/step - loss: 1.0791 - profit_accuracy_112: 0.3813 - val_loss: 1.0862 - val_profit_accuracy_112: 0.4444\n",
      "Epoch 76/100\n",
      "118/118 [==============================] - 1s 5ms/step - loss: 1.0802 - profit_accuracy_112: 0.3863 - val_loss: 1.0936 - val_profit_accuracy_112: 0.3430\n",
      "Epoch 77/100\n",
      "118/118 [==============================] - 1s 6ms/step - loss: 1.0795 - profit_accuracy_112: 0.3921 - val_loss: 1.0909 - val_profit_accuracy_112: 0.3623\n",
      "Epoch 78/100\n",
      "118/118 [==============================] - 1s 6ms/step - loss: 1.0787 - profit_accuracy_112: 0.3922 - val_loss: 1.0874 - val_profit_accuracy_112: 0.3950\n",
      "Epoch 79/100\n",
      "118/118 [==============================] - 1s 6ms/step - loss: 1.0793 - profit_accuracy_112: 0.3817 - val_loss: 1.0861 - val_profit_accuracy_112: 0.4091\n",
      "Epoch 80/100\n",
      "118/118 [==============================] - 0s 4ms/step - loss: 1.0782 - profit_accuracy_112: 0.3973 - val_loss: 1.0888 - val_profit_accuracy_112: 0.3667\n",
      "Epoch 81/100\n",
      "118/118 [==============================] - 1s 5ms/step - loss: 1.0790 - profit_accuracy_112: 0.3857 - val_loss: 1.0867 - val_profit_accuracy_112: 0.3516\n",
      "Epoch 82/100\n",
      "118/118 [==============================] - 1s 6ms/step - loss: 1.0785 - profit_accuracy_112: 0.3829 - val_loss: 1.0878 - val_profit_accuracy_112: 0.3708\n",
      "Epoch 83/100\n",
      "118/118 [==============================] - 1s 5ms/step - loss: 1.0787 - profit_accuracy_112: 0.3935 - val_loss: 1.0873 - val_profit_accuracy_112: 0.3681\n",
      "Epoch 84/100\n",
      "118/118 [==============================] - 1s 4ms/step - loss: 1.0796 - profit_accuracy_112: 0.3872 - val_loss: 1.0873 - val_profit_accuracy_112: 0.3750\n",
      "Epoch 85/100\n",
      "118/118 [==============================] - 1s 5ms/step - loss: 1.0782 - profit_accuracy_112: 0.3903 - val_loss: 1.0857 - val_profit_accuracy_112: 0.4176\n",
      "Epoch 86/100\n",
      "118/118 [==============================] - 1s 7ms/step - loss: 1.0797 - profit_accuracy_112: 0.3804 - val_loss: 1.0868 - val_profit_accuracy_112: 0.3721\n",
      "Epoch 87/100\n",
      "118/118 [==============================] - 1s 6ms/step - loss: 1.0780 - profit_accuracy_112: 0.3765 - val_loss: 1.0864 - val_profit_accuracy_112: 0.4337\n",
      "Epoch 88/100\n",
      "118/118 [==============================] - 1s 8ms/step - loss: 1.0779 - profit_accuracy_112: 0.3904 - val_loss: 1.0921 - val_profit_accuracy_112: 0.3528\n",
      "Epoch 89/100\n",
      "118/118 [==============================] - 1s 7ms/step - loss: 1.0801 - profit_accuracy_112: 0.3778 - val_loss: 1.0861 - val_profit_accuracy_112: 0.3810\n",
      "Epoch 90/100\n",
      "118/118 [==============================] - 1s 5ms/step - loss: 1.0779 - profit_accuracy_112: 0.3934 - val_loss: 1.0862 - val_profit_accuracy_112: 0.4359\n",
      "Epoch 91/100\n",
      "118/118 [==============================] - 1s 5ms/step - loss: 1.0790 - profit_accuracy_112: 0.3788 - val_loss: 1.0865 - val_profit_accuracy_112: 0.4040\n",
      "Epoch 92/100\n",
      "118/118 [==============================] - 1s 5ms/step - loss: 1.0784 - profit_accuracy_112: 0.3915 - val_loss: 1.0873 - val_profit_accuracy_112: 0.3934\n",
      "Epoch 93/100\n",
      "118/118 [==============================] - 1s 5ms/step - loss: 1.0795 - profit_accuracy_112: 0.3819 - val_loss: 1.0866 - val_profit_accuracy_112: 0.4118\n",
      "Epoch 94/100\n",
      "118/118 [==============================] - 1s 5ms/step - loss: 1.0781 - profit_accuracy_112: 0.3994 - val_loss: 1.0860 - val_profit_accuracy_112: 0.3944\n",
      "Epoch 95/100\n",
      "118/118 [==============================] - 1s 4ms/step - loss: 1.0772 - profit_accuracy_112: 0.3831 - val_loss: 1.0889 - val_profit_accuracy_112: 0.3655\n",
      "Epoch 96/100\n",
      "118/118 [==============================] - 1s 5ms/step - loss: 1.0785 - profit_accuracy_112: 0.3813 - val_loss: 1.0865 - val_profit_accuracy_112: 0.4368\n",
      "Epoch 97/100\n",
      "118/118 [==============================] - 1s 6ms/step - loss: 1.0779 - profit_accuracy_112: 0.3883 - val_loss: 1.0879 - val_profit_accuracy_112: 0.3750\n",
      "Epoch 98/100\n",
      "118/118 [==============================] - 1s 6ms/step - loss: 1.0779 - profit_accuracy_112: 0.3863 - val_loss: 1.0936 - val_profit_accuracy_112: 0.3430\n",
      "Epoch 99/100\n",
      "118/118 [==============================] - 1s 5ms/step - loss: 1.0768 - profit_accuracy_112: 0.3859 - val_loss: 1.0897 - val_profit_accuracy_112: 0.3563\n",
      "Epoch 100/100\n",
      "118/118 [==============================] - 1s 5ms/step - loss: 1.0782 - profit_accuracy_112: 0.3914 - val_loss: 1.0860 - val_profit_accuracy_112: 0.3810\n",
      "30/30 [==============================] - 1s 3ms/step\n",
      "Profit_accuracy = 0.380952388048172\n"
     ]
    }
   ],
   "source": [
    "y_tech = data_tech['target']\n",
    "X_tech = data_tech.drop(columns=['target'])\n",
    "\n",
    "X_train_normalized_tech, X_test_normalized_tech, y_train, y_test = data_normalized(X_tech, y_tech)\n",
    "y_pred, profit_accuracy = train_lstm_model(X_train_normalized_tech, X_test_normalized_tech, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1889,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_predictions(me_pred_probs, ti_pred_probs):\n",
    "    combined_pred_probs = []\n",
    "\n",
    "    for me_probs, ti_probs in zip(me_pred_probs, ti_pred_probs):\n",
    "        # If one model's prediction is class_noact, set final decision as class_noact\n",
    "        if np.argmax(me_probs) == 0 or np.argmax(ti_probs) == 0:\n",
    "            combined_pred_probs.append([1, 0, 0])  # class_noact one-hot encoding\n",
    "        # If both models agree on labels, set final decision as this label\n",
    "        elif np.argmax(me_probs) == np.argmax(ti_probs):\n",
    "            combined_pred_probs.append(me_probs)  # Use either model's probabilities\n",
    "        # If predictions of the two models are different, choose the one with higher probability\n",
    "        else:\n",
    "            max_prob_index = np.argmax([np.max(me_probs), np.max(ti_probs)])\n",
    "            if max_prob_index == 0:\n",
    "                combined_pred_probs.append(me_probs)  # Use ME_LSTM's probabilities\n",
    "            else:\n",
    "                combined_pred_probs.append(ti_probs)  # Use TI_LSTM's probabilities\n",
    "\n",
    "    return np.array(combined_pred_probs)\n",
    "\n",
    "\n",
    "def train_lstm_model_combine(X_train_eco, X_test_eco, X_train_tech, X_test_tech, y_train, y_test, lstm_units=lstm_units, epochs=epochs, batch_size=batch_size):\n",
    "    # Train ME_LSTM model\n",
    "    me_pred_probs, _ = train_lstm_model(X_train_eco, X_test_eco, y_train, y_test, lstm_units, epochs, batch_size)\n",
    "\n",
    "    # Train TI_LSTM model\n",
    "    ti_pred_probs, _ = train_lstm_model(X_train_tech, X_test_tech, y_train, y_test, lstm_units, epochs, batch_size)\n",
    "\n",
    "    # Combine predictions\n",
    "    combined_pred_probs = combine_predictions(me_pred_probs, ti_pred_probs)\n",
    "\n",
    "    # Convert y_test to one-hot encoding\n",
    "    encoder = LabelEncoder()\n",
    "    encoder.fit(y_train)\n",
    "    encoded_y_test = encoder.transform(y_test)\n",
    "    y_test_onehot = to_categorical(encoded_y_test)\n",
    "\n",
    "    # Evaluate the combined model using ProfitAccuracy metric\n",
    "    profit_accuracy = ProfitAccuracy()(y_test_onehot, combined_pred_probs)\n",
    "    print(f\"Combine profit_accuracy = {profit_accuracy}\")\n",
    "\n",
    "    return combined_pred_probs, profit_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1891,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "118/118 [==============================] - 3s 10ms/step - loss: 1.0924 - profit_accuracy_119: 0.3296 - val_loss: 1.0815 - val_profit_accuracy_119: 0.0000e+00\n",
      "Epoch 2/100\n",
      "118/118 [==============================] - 1s 5ms/step - loss: 1.0827 - profit_accuracy_119: 0.3822 - val_loss: 1.0884 - val_profit_accuracy_119: 0.0000e+00\n",
      "Epoch 3/100\n",
      "118/118 [==============================] - 1s 5ms/step - loss: 1.0803 - profit_accuracy_119: 0.3652 - val_loss: 1.1014 - val_profit_accuracy_119: 0.0000e+00\n",
      "Epoch 4/100\n",
      "118/118 [==============================] - 1s 5ms/step - loss: 1.0797 - profit_accuracy_119: 0.3726 - val_loss: 1.1230 - val_profit_accuracy_119: 0.0000e+00\n",
      "Epoch 5/100\n",
      "118/118 [==============================] - 1s 5ms/step - loss: 1.0785 - profit_accuracy_119: 0.3773 - val_loss: 1.1147 - val_profit_accuracy_119: 0.0000e+00\n",
      "Epoch 6/100\n",
      "118/118 [==============================] - 1s 6ms/step - loss: 1.0773 - profit_accuracy_119: 0.3837 - val_loss: 1.0960 - val_profit_accuracy_119: 0.0000e+00\n",
      "Epoch 7/100\n",
      "118/118 [==============================] - 1s 5ms/step - loss: 1.0776 - profit_accuracy_119: 0.3801 - val_loss: 1.1188 - val_profit_accuracy_119: 0.0000e+00\n",
      "Epoch 8/100\n",
      "118/118 [==============================] - 1s 5ms/step - loss: 1.0772 - profit_accuracy_119: 0.3951 - val_loss: 1.1152 - val_profit_accuracy_119: 0.0000e+00\n",
      "Epoch 9/100\n",
      "118/118 [==============================] - 1s 5ms/step - loss: 1.0765 - profit_accuracy_119: 0.3889 - val_loss: 1.1555 - val_profit_accuracy_119: 0.0000e+00\n",
      "Epoch 10/100\n",
      "118/118 [==============================] - 1s 6ms/step - loss: 1.0769 - profit_accuracy_119: 0.3849 - val_loss: 1.1064 - val_profit_accuracy_119: 0.0000e+00\n",
      "Epoch 11/100\n",
      "118/118 [==============================] - 1s 5ms/step - loss: 1.0760 - profit_accuracy_119: 0.3875 - val_loss: 1.1338 - val_profit_accuracy_119: 0.0000e+00\n",
      "Epoch 12/100\n",
      "118/118 [==============================] - 1s 5ms/step - loss: 1.0755 - profit_accuracy_119: 0.3852 - val_loss: 1.1297 - val_profit_accuracy_119: 0.0000e+00\n",
      "Epoch 13/100\n",
      "118/118 [==============================] - 1s 6ms/step - loss: 1.0757 - profit_accuracy_119: 0.3882 - val_loss: 1.1146 - val_profit_accuracy_119: 0.0000e+00\n",
      "Epoch 14/100\n",
      "118/118 [==============================] - 1s 6ms/step - loss: 1.0751 - profit_accuracy_119: 0.3895 - val_loss: 1.1438 - val_profit_accuracy_119: 0.0000e+00\n",
      "Epoch 15/100\n",
      "118/118 [==============================] - 1s 6ms/step - loss: 1.0746 - profit_accuracy_119: 0.3882 - val_loss: 1.1735 - val_profit_accuracy_119: 0.0000e+00\n",
      "Epoch 16/100\n",
      "118/118 [==============================] - 1s 6ms/step - loss: 1.0761 - profit_accuracy_119: 0.3885 - val_loss: 1.0988 - val_profit_accuracy_119: 0.4179\n",
      "Epoch 17/100\n",
      "118/118 [==============================] - 1s 5ms/step - loss: 1.0743 - profit_accuracy_119: 0.4022 - val_loss: 1.1488 - val_profit_accuracy_119: 0.0000e+00\n",
      "Epoch 18/100\n",
      "118/118 [==============================] - 1s 5ms/step - loss: 1.0744 - profit_accuracy_119: 0.3945 - val_loss: 1.1230 - val_profit_accuracy_119: 0.2500\n",
      "Epoch 19/100\n",
      "118/118 [==============================] - 1s 5ms/step - loss: 1.0741 - profit_accuracy_119: 0.3961 - val_loss: 1.1782 - val_profit_accuracy_119: 0.0000e+00\n",
      "Epoch 20/100\n",
      "118/118 [==============================] - 1s 5ms/step - loss: 1.0729 - profit_accuracy_119: 0.4074 - val_loss: 1.1372 - val_profit_accuracy_119: 0.0000e+00\n",
      "Epoch 21/100\n",
      "118/118 [==============================] - 1s 6ms/step - loss: 1.0736 - profit_accuracy_119: 0.3901 - val_loss: 1.1562 - val_profit_accuracy_119: 0.0000e+00\n",
      "Epoch 22/100\n",
      "118/118 [==============================] - 1s 7ms/step - loss: 1.0735 - profit_accuracy_119: 0.3850 - val_loss: 1.1375 - val_profit_accuracy_119: 0.3125\n",
      "Epoch 23/100\n",
      "118/118 [==============================] - 1s 9ms/step - loss: 1.0728 - profit_accuracy_119: 0.3858 - val_loss: 1.1515 - val_profit_accuracy_119: 0.2500\n",
      "Epoch 24/100\n",
      "118/118 [==============================] - 1s 9ms/step - loss: 1.0728 - profit_accuracy_119: 0.3847 - val_loss: 1.1587 - val_profit_accuracy_119: 0.2857\n",
      "Epoch 25/100\n",
      "118/118 [==============================] - 1s 8ms/step - loss: 1.0722 - profit_accuracy_119: 0.3955 - val_loss: 1.1470 - val_profit_accuracy_119: 0.4074\n",
      "Epoch 26/100\n",
      "118/118 [==============================] - 1s 8ms/step - loss: 1.0732 - profit_accuracy_119: 0.3901 - val_loss: 1.1488 - val_profit_accuracy_119: 0.0000e+00\n",
      "Epoch 27/100\n",
      "118/118 [==============================] - 1s 7ms/step - loss: 1.0715 - profit_accuracy_119: 0.3928 - val_loss: 1.1217 - val_profit_accuracy_119: 0.3830\n",
      "Epoch 28/100\n",
      "118/118 [==============================] - 1s 8ms/step - loss: 1.0731 - profit_accuracy_119: 0.3911 - val_loss: 1.1666 - val_profit_accuracy_119: 0.0000e+00\n",
      "Epoch 29/100\n",
      "118/118 [==============================] - 1s 7ms/step - loss: 1.0723 - profit_accuracy_119: 0.3886 - val_loss: 1.1642 - val_profit_accuracy_119: 0.0000e+00\n",
      "Epoch 30/100\n",
      "118/118 [==============================] - 1s 7ms/step - loss: 1.0726 - profit_accuracy_119: 0.3816 - val_loss: 1.1484 - val_profit_accuracy_119: 0.2632\n",
      "Epoch 31/100\n",
      "118/118 [==============================] - 1s 6ms/step - loss: 1.0722 - profit_accuracy_119: 0.4009 - val_loss: 1.1815 - val_profit_accuracy_119: 0.0000e+00\n",
      "Epoch 32/100\n",
      "118/118 [==============================] - 1s 6ms/step - loss: 1.0729 - profit_accuracy_119: 0.3946 - val_loss: 1.1641 - val_profit_accuracy_119: 0.2857\n",
      "Epoch 33/100\n",
      "118/118 [==============================] - 1s 7ms/step - loss: 1.0716 - profit_accuracy_119: 0.3856 - val_loss: 1.2005 - val_profit_accuracy_119: 0.0000e+00\n",
      "Epoch 34/100\n",
      "118/118 [==============================] - 1s 7ms/step - loss: 1.0724 - profit_accuracy_119: 0.3965 - val_loss: 1.1692 - val_profit_accuracy_119: 0.3125\n",
      "Epoch 35/100\n",
      "118/118 [==============================] - 1s 6ms/step - loss: 1.0711 - profit_accuracy_119: 0.3926 - val_loss: 1.1663 - val_profit_accuracy_119: 0.2500\n",
      "Epoch 36/100\n",
      "118/118 [==============================] - 1s 8ms/step - loss: 1.0723 - profit_accuracy_119: 0.3880 - val_loss: 1.1614 - val_profit_accuracy_119: 0.3077\n",
      "Epoch 37/100\n",
      "118/118 [==============================] - 1s 6ms/step - loss: 1.0716 - profit_accuracy_119: 0.3961 - val_loss: 1.1711 - val_profit_accuracy_119: 0.3125\n",
      "Epoch 38/100\n",
      "118/118 [==============================] - 1s 5ms/step - loss: 1.0715 - profit_accuracy_119: 0.3913 - val_loss: 1.2332 - val_profit_accuracy_119: 0.0000e+00\n",
      "Epoch 39/100\n",
      "118/118 [==============================] - 1s 5ms/step - loss: 1.0716 - profit_accuracy_119: 0.3954 - val_loss: 1.1892 - val_profit_accuracy_119: 0.3077\n",
      "Epoch 40/100\n",
      "118/118 [==============================] - 1s 6ms/step - loss: 1.0708 - profit_accuracy_119: 0.3983 - val_loss: 1.2947 - val_profit_accuracy_119: 0.0000e+00\n",
      "Epoch 41/100\n",
      "118/118 [==============================] - 1s 5ms/step - loss: 1.0713 - profit_accuracy_119: 0.3969 - val_loss: 1.1898 - val_profit_accuracy_119: 0.0000e+00\n",
      "Epoch 42/100\n",
      "118/118 [==============================] - 1s 5ms/step - loss: 1.0716 - profit_accuracy_119: 0.3995 - val_loss: 1.1977 - val_profit_accuracy_119: 0.0000e+00\n",
      "Epoch 43/100\n",
      "118/118 [==============================] - 1s 5ms/step - loss: 1.0709 - profit_accuracy_119: 0.3967 - val_loss: 1.1988 - val_profit_accuracy_119: 0.3333\n",
      "Epoch 44/100\n",
      "118/118 [==============================] - 1s 6ms/step - loss: 1.0712 - profit_accuracy_119: 0.3862 - val_loss: 1.2154 - val_profit_accuracy_119: 0.0000e+00\n",
      "Epoch 45/100\n",
      "118/118 [==============================] - 1s 5ms/step - loss: 1.0709 - profit_accuracy_119: 0.3930 - val_loss: 1.1642 - val_profit_accuracy_119: 0.3500\n",
      "Epoch 46/100\n",
      "118/118 [==============================] - 1s 5ms/step - loss: 1.0704 - profit_accuracy_119: 0.4006 - val_loss: 1.2665 - val_profit_accuracy_119: 0.0000e+00\n",
      "Epoch 47/100\n",
      "118/118 [==============================] - 1s 5ms/step - loss: 1.0714 - profit_accuracy_119: 0.3963 - val_loss: 1.1973 - val_profit_accuracy_119: 0.0000e+00\n",
      "Epoch 48/100\n",
      "118/118 [==============================] - 1s 5ms/step - loss: 1.0704 - profit_accuracy_119: 0.3948 - val_loss: 1.1763 - val_profit_accuracy_119: 0.2857\n",
      "Epoch 49/100\n",
      "118/118 [==============================] - 1s 5ms/step - loss: 1.0705 - profit_accuracy_119: 0.3938 - val_loss: 1.2034 - val_profit_accuracy_119: 0.0000e+00\n",
      "Epoch 50/100\n",
      "118/118 [==============================] - 1s 6ms/step - loss: 1.0715 - profit_accuracy_119: 0.3915 - val_loss: 1.1834 - val_profit_accuracy_119: 0.3125\n",
      "Epoch 51/100\n",
      "118/118 [==============================] - 1s 6ms/step - loss: 1.0706 - profit_accuracy_119: 0.3960 - val_loss: 1.1859 - val_profit_accuracy_119: 0.3500\n",
      "Epoch 52/100\n",
      "118/118 [==============================] - 1s 6ms/step - loss: 1.0711 - profit_accuracy_119: 0.4099 - val_loss: 1.1797 - val_profit_accuracy_119: 0.3158\n",
      "Epoch 53/100\n",
      "118/118 [==============================] - 1s 5ms/step - loss: 1.0707 - profit_accuracy_119: 0.3985 - val_loss: 1.1921 - val_profit_accuracy_119: 0.2857\n",
      "Epoch 54/100\n",
      "118/118 [==============================] - 1s 5ms/step - loss: 1.0704 - profit_accuracy_119: 0.3941 - val_loss: 1.1711 - val_profit_accuracy_119: 0.3158\n",
      "Epoch 55/100\n",
      "118/118 [==============================] - 1s 5ms/step - loss: 1.0701 - profit_accuracy_119: 0.3940 - val_loss: 1.2083 - val_profit_accuracy_119: 0.2857\n",
      "Epoch 56/100\n",
      "118/118 [==============================] - 1s 5ms/step - loss: 1.0700 - profit_accuracy_119: 0.3958 - val_loss: 1.2121 - val_profit_accuracy_119: 0.0000e+00\n",
      "Epoch 57/100\n",
      "118/118 [==============================] - 1s 5ms/step - loss: 1.0727 - profit_accuracy_119: 0.3927 - val_loss: 1.1666 - val_profit_accuracy_119: 0.3158\n",
      "Epoch 58/100\n",
      "118/118 [==============================] - 1s 5ms/step - loss: 1.0709 - profit_accuracy_119: 0.3978 - val_loss: 1.1751 - val_profit_accuracy_119: 0.3333\n",
      "Epoch 59/100\n",
      "118/118 [==============================] - 1s 5ms/step - loss: 1.0714 - profit_accuracy_119: 0.3971 - val_loss: 1.1379 - val_profit_accuracy_119: 0.3578\n",
      "Epoch 60/100\n",
      "118/118 [==============================] - 1s 6ms/step - loss: 1.0702 - profit_accuracy_119: 0.3958 - val_loss: 1.1763 - val_profit_accuracy_119: 0.2692\n",
      "Epoch 61/100\n",
      "118/118 [==============================] - 1s 5ms/step - loss: 1.0703 - profit_accuracy_119: 0.3954 - val_loss: 1.2206 - val_profit_accuracy_119: 0.3333\n",
      "Epoch 62/100\n",
      "118/118 [==============================] - 1s 5ms/step - loss: 1.0705 - profit_accuracy_119: 0.4039 - val_loss: 1.2133 - val_profit_accuracy_119: 0.0000e+00\n",
      "Epoch 63/100\n",
      "118/118 [==============================] - 1s 6ms/step - loss: 1.0712 - profit_accuracy_119: 0.3983 - val_loss: 1.1847 - val_profit_accuracy_119: 0.3158\n",
      "Epoch 64/100\n",
      "118/118 [==============================] - 1s 6ms/step - loss: 1.0704 - profit_accuracy_119: 0.3957 - val_loss: 1.1590 - val_profit_accuracy_119: 0.4146\n",
      "Epoch 65/100\n",
      "118/118 [==============================] - 1s 5ms/step - loss: 1.0694 - profit_accuracy_119: 0.3908 - val_loss: 1.2242 - val_profit_accuracy_119: 0.0000e+00\n",
      "Epoch 66/100\n",
      "118/118 [==============================] - 1s 5ms/step - loss: 1.0703 - profit_accuracy_119: 0.3972 - val_loss: 1.2595 - val_profit_accuracy_119: 0.0000e+00\n",
      "Epoch 67/100\n",
      "118/118 [==============================] - 1s 5ms/step - loss: 1.0711 - profit_accuracy_119: 0.3991 - val_loss: 1.1893 - val_profit_accuracy_119: 0.3333\n",
      "Epoch 68/100\n",
      "118/118 [==============================] - 1s 5ms/step - loss: 1.0701 - profit_accuracy_119: 0.3954 - val_loss: 1.2051 - val_profit_accuracy_119: 0.2500\n",
      "Epoch 69/100\n",
      "118/118 [==============================] - 1s 5ms/step - loss: 1.0698 - profit_accuracy_119: 0.3944 - val_loss: 1.1593 - val_profit_accuracy_119: 0.3077\n",
      "Epoch 70/100\n",
      "118/118 [==============================] - 1s 5ms/step - loss: 1.0699 - profit_accuracy_119: 0.3950 - val_loss: 1.2019 - val_profit_accuracy_119: 0.2500\n",
      "Epoch 71/100\n",
      "118/118 [==============================] - 1s 6ms/step - loss: 1.0697 - profit_accuracy_119: 0.3977 - val_loss: 1.1571 - val_profit_accuracy_119: 0.3125\n",
      "Epoch 72/100\n",
      "118/118 [==============================] - 1s 6ms/step - loss: 1.0698 - profit_accuracy_119: 0.3981 - val_loss: 1.2068 - val_profit_accuracy_119: 0.2857\n",
      "Epoch 73/100\n",
      "118/118 [==============================] - 1s 7ms/step - loss: 1.0697 - profit_accuracy_119: 0.4041 - val_loss: 1.2206 - val_profit_accuracy_119: 0.3636\n",
      "Epoch 74/100\n",
      "118/118 [==============================] - 1s 6ms/step - loss: 1.0700 - profit_accuracy_119: 0.4045 - val_loss: 1.1919 - val_profit_accuracy_119: 0.3158\n",
      "Epoch 75/100\n",
      "118/118 [==============================] - 1s 5ms/step - loss: 1.0699 - profit_accuracy_119: 0.4054 - val_loss: 1.1632 - val_profit_accuracy_119: 0.2542\n",
      "Epoch 76/100\n",
      "118/118 [==============================] - 1s 5ms/step - loss: 1.0697 - profit_accuracy_119: 0.3940 - val_loss: 1.2294 - val_profit_accuracy_119: 0.0000e+00\n",
      "Epoch 77/100\n",
      "118/118 [==============================] - 1s 6ms/step - loss: 1.0706 - profit_accuracy_119: 0.3985 - val_loss: 1.1765 - val_profit_accuracy_119: 0.3750\n",
      "Epoch 78/100\n",
      "118/118 [==============================] - 1s 5ms/step - loss: 1.0693 - profit_accuracy_119: 0.3941 - val_loss: 1.2157 - val_profit_accuracy_119: 0.3333\n",
      "Epoch 79/100\n",
      "118/118 [==============================] - 1s 6ms/step - loss: 1.0697 - profit_accuracy_119: 0.4009 - val_loss: 1.1802 - val_profit_accuracy_119: 0.3333\n",
      "Epoch 80/100\n",
      "118/118 [==============================] - 1s 5ms/step - loss: 1.0692 - profit_accuracy_119: 0.4020 - val_loss: 1.1373 - val_profit_accuracy_119: 0.2871\n",
      "Epoch 81/100\n",
      "118/118 [==============================] - 1s 6ms/step - loss: 1.0696 - profit_accuracy_119: 0.3990 - val_loss: 1.1933 - val_profit_accuracy_119: 0.3333\n",
      "Epoch 82/100\n",
      "118/118 [==============================] - 1s 5ms/step - loss: 1.0698 - profit_accuracy_119: 0.4003 - val_loss: 1.1889 - val_profit_accuracy_119: 0.2857\n",
      "Epoch 83/100\n",
      "118/118 [==============================] - 1s 5ms/step - loss: 1.0692 - profit_accuracy_119: 0.3954 - val_loss: 1.2038 - val_profit_accuracy_119: 0.3333\n",
      "Epoch 84/100\n",
      "118/118 [==============================] - 1s 5ms/step - loss: 1.0690 - profit_accuracy_119: 0.4030 - val_loss: 1.2113 - val_profit_accuracy_119: 0.4000\n",
      "Epoch 85/100\n",
      "118/118 [==============================] - 1s 6ms/step - loss: 1.0691 - profit_accuracy_119: 0.3946 - val_loss: 1.1678 - val_profit_accuracy_119: 0.2826\n",
      "Epoch 86/100\n",
      "118/118 [==============================] - 1s 6ms/step - loss: 1.0700 - profit_accuracy_119: 0.4002 - val_loss: 1.1994 - val_profit_accuracy_119: 0.2500\n",
      "Epoch 87/100\n",
      "118/118 [==============================] - 1s 6ms/step - loss: 1.0693 - profit_accuracy_119: 0.3996 - val_loss: 1.1991 - val_profit_accuracy_119: 0.3000\n",
      "Epoch 88/100\n",
      "118/118 [==============================] - 1s 6ms/step - loss: 1.0692 - profit_accuracy_119: 0.3997 - val_loss: 1.1668 - val_profit_accuracy_119: 0.3250\n",
      "Epoch 89/100\n",
      "118/118 [==============================] - 1s 6ms/step - loss: 1.0688 - profit_accuracy_119: 0.4032 - val_loss: 1.1819 - val_profit_accuracy_119: 0.3333\n",
      "Epoch 90/100\n",
      "118/118 [==============================] - 1s 6ms/step - loss: 1.0688 - profit_accuracy_119: 0.4049 - val_loss: 1.1535 - val_profit_accuracy_119: 0.2329\n",
      "Epoch 91/100\n",
      "118/118 [==============================] - 1s 6ms/step - loss: 1.0697 - profit_accuracy_119: 0.4037 - val_loss: 1.1967 - val_profit_accuracy_119: 0.3333\n",
      "Epoch 92/100\n",
      "118/118 [==============================] - 1s 6ms/step - loss: 1.0691 - profit_accuracy_119: 0.4036 - val_loss: 1.1770 - val_profit_accuracy_119: 0.2857\n",
      "Epoch 93/100\n",
      "118/118 [==============================] - 1s 5ms/step - loss: 1.0692 - profit_accuracy_119: 0.3980 - val_loss: 1.1846 - val_profit_accuracy_119: 0.3529\n",
      "Epoch 94/100\n",
      "118/118 [==============================] - 1s 5ms/step - loss: 1.0686 - profit_accuracy_119: 0.3978 - val_loss: 1.1884 - val_profit_accuracy_119: 0.3500\n",
      "Epoch 95/100\n",
      "118/118 [==============================] - 1s 6ms/step - loss: 1.0694 - profit_accuracy_119: 0.4042 - val_loss: 1.1754 - val_profit_accuracy_119: 0.3333\n",
      "Epoch 96/100\n",
      "118/118 [==============================] - 1s 5ms/step - loss: 1.0686 - profit_accuracy_119: 0.3934 - val_loss: 1.2045 - val_profit_accuracy_119: 0.4000\n",
      "Epoch 97/100\n",
      "118/118 [==============================] - 1s 6ms/step - loss: 1.0689 - profit_accuracy_119: 0.4076 - val_loss: 1.1871 - val_profit_accuracy_119: 0.3125\n",
      "Epoch 98/100\n",
      "118/118 [==============================] - 1s 6ms/step - loss: 1.0686 - profit_accuracy_119: 0.3982 - val_loss: 1.1292 - val_profit_accuracy_119: 0.3221\n",
      "Epoch 99/100\n",
      "118/118 [==============================] - 1s 5ms/step - loss: 1.0687 - profit_accuracy_119: 0.4102 - val_loss: 1.1863 - val_profit_accuracy_119: 0.0000e+00\n",
      "Epoch 100/100\n",
      "118/118 [==============================] - 1s 6ms/step - loss: 1.0688 - profit_accuracy_119: 0.4122 - val_loss: 1.1601 - val_profit_accuracy_119: 0.3281\n",
      "30/30 [==============================] - 0s 2ms/step\n",
      "Profit_accuracy = 0.328125\n",
      "Epoch 1/100\n",
      "118/118 [==============================] - 3s 9ms/step - loss: 1.0961 - profit_accuracy_121: 0.3366 - val_loss: 1.1019 - val_profit_accuracy_121: 0.2614\n",
      "Epoch 2/100\n",
      "118/118 [==============================] - 1s 5ms/step - loss: 1.0928 - profit_accuracy_121: 0.3469 - val_loss: 1.1038 - val_profit_accuracy_121: 0.2608\n",
      "Epoch 3/100\n",
      "118/118 [==============================] - 1s 4ms/step - loss: 1.0923 - profit_accuracy_121: 0.3560 - val_loss: 1.0970 - val_profit_accuracy_121: 0.2866\n",
      "Epoch 4/100\n",
      "118/118 [==============================] - 1s 5ms/step - loss: 1.0919 - profit_accuracy_121: 0.3500 - val_loss: 1.0969 - val_profit_accuracy_121: 0.2853\n",
      "Epoch 5/100\n",
      "118/118 [==============================] - 1s 6ms/step - loss: 1.0915 - profit_accuracy_121: 0.3632 - val_loss: 1.0919 - val_profit_accuracy_121: 0.0000e+00\n",
      "Epoch 6/100\n",
      "118/118 [==============================] - 1s 6ms/step - loss: 1.0911 - profit_accuracy_121: 0.3660 - val_loss: 1.0969 - val_profit_accuracy_121: 0.2816\n",
      "Epoch 7/100\n",
      "118/118 [==============================] - 1s 6ms/step - loss: 1.0912 - profit_accuracy_121: 0.3727 - val_loss: 1.0936 - val_profit_accuracy_121: 0.2794\n",
      "Epoch 8/100\n",
      "118/118 [==============================] - 1s 8ms/step - loss: 1.0901 - profit_accuracy_121: 0.3689 - val_loss: 1.0912 - val_profit_accuracy_121: 0.0000e+00\n",
      "Epoch 9/100\n",
      "118/118 [==============================] - 1s 6ms/step - loss: 1.0900 - profit_accuracy_121: 0.3742 - val_loss: 1.0954 - val_profit_accuracy_121: 0.3166\n",
      "Epoch 10/100\n",
      "118/118 [==============================] - 1s 5ms/step - loss: 1.0898 - profit_accuracy_121: 0.3794 - val_loss: 1.0930 - val_profit_accuracy_121: 0.3017\n",
      "Epoch 11/100\n",
      "118/118 [==============================] - 1s 5ms/step - loss: 1.0894 - profit_accuracy_121: 0.3708 - val_loss: 1.0908 - val_profit_accuracy_121: 0.0000e+00\n",
      "Epoch 12/100\n",
      "118/118 [==============================] - 1s 6ms/step - loss: 1.0893 - profit_accuracy_121: 0.3709 - val_loss: 1.0915 - val_profit_accuracy_121: 0.3226\n",
      "Epoch 13/100\n",
      "118/118 [==============================] - 1s 6ms/step - loss: 1.0896 - profit_accuracy_121: 0.3790 - val_loss: 1.0944 - val_profit_accuracy_121: 0.2705\n",
      "Epoch 14/100\n",
      "118/118 [==============================] - 1s 5ms/step - loss: 1.0898 - profit_accuracy_121: 0.3615 - val_loss: 1.0918 - val_profit_accuracy_121: 0.2609\n",
      "Epoch 15/100\n",
      "118/118 [==============================] - 1s 5ms/step - loss: 1.0896 - profit_accuracy_121: 0.3634 - val_loss: 1.0918 - val_profit_accuracy_121: 0.3059\n",
      "Epoch 16/100\n",
      "118/118 [==============================] - 1s 5ms/step - loss: 1.0883 - profit_accuracy_121: 0.3718 - val_loss: 1.0919 - val_profit_accuracy_121: 0.2963\n",
      "Epoch 17/100\n",
      "118/118 [==============================] - 1s 5ms/step - loss: 1.0893 - profit_accuracy_121: 0.3664 - val_loss: 1.0899 - val_profit_accuracy_121: 0.2500\n",
      "Epoch 18/100\n",
      "118/118 [==============================] - 1s 5ms/step - loss: 1.0881 - profit_accuracy_121: 0.3724 - val_loss: 1.0921 - val_profit_accuracy_121: 0.2932\n",
      "Epoch 19/100\n",
      "118/118 [==============================] - 1s 6ms/step - loss: 1.0878 - profit_accuracy_121: 0.3814 - val_loss: 1.0891 - val_profit_accuracy_121: 0.3226\n",
      "Epoch 20/100\n",
      "118/118 [==============================] - 1s 5ms/step - loss: 1.0877 - profit_accuracy_121: 0.3726 - val_loss: 1.0907 - val_profit_accuracy_121: 0.2755\n",
      "Epoch 21/100\n",
      "118/118 [==============================] - 1s 6ms/step - loss: 1.0873 - profit_accuracy_121: 0.3768 - val_loss: 1.0924 - val_profit_accuracy_121: 0.3234\n",
      "Epoch 22/100\n",
      "118/118 [==============================] - 1s 6ms/step - loss: 1.0870 - profit_accuracy_121: 0.3839 - val_loss: 1.0889 - val_profit_accuracy_121: 0.3429\n",
      "Epoch 23/100\n",
      "118/118 [==============================] - 1s 6ms/step - loss: 1.0865 - profit_accuracy_121: 0.3818 - val_loss: 1.0891 - val_profit_accuracy_121: 0.3019\n",
      "Epoch 24/100\n",
      "118/118 [==============================] - 1s 5ms/step - loss: 1.0866 - profit_accuracy_121: 0.3835 - val_loss: 1.0928 - val_profit_accuracy_121: 0.3498\n",
      "Epoch 25/100\n",
      "118/118 [==============================] - 1s 5ms/step - loss: 1.0865 - profit_accuracy_121: 0.3817 - val_loss: 1.0904 - val_profit_accuracy_121: 0.3571\n",
      "Epoch 26/100\n",
      "118/118 [==============================] - 1s 4ms/step - loss: 1.0870 - profit_accuracy_121: 0.3787 - val_loss: 1.0894 - val_profit_accuracy_121: 0.3171\n",
      "Epoch 27/100\n",
      "118/118 [==============================] - 1s 5ms/step - loss: 1.0860 - profit_accuracy_121: 0.3717 - val_loss: 1.0964 - val_profit_accuracy_121: 0.3192\n",
      "Epoch 28/100\n",
      "118/118 [==============================] - 1s 4ms/step - loss: 1.0873 - profit_accuracy_121: 0.3753 - val_loss: 1.0911 - val_profit_accuracy_121: 0.3535\n",
      "Epoch 29/100\n",
      "118/118 [==============================] - 0s 4ms/step - loss: 1.0868 - profit_accuracy_121: 0.3908 - val_loss: 1.0897 - val_profit_accuracy_121: 0.3788\n",
      "Epoch 30/100\n",
      "118/118 [==============================] - 1s 5ms/step - loss: 1.0855 - profit_accuracy_121: 0.3803 - val_loss: 1.0903 - val_profit_accuracy_121: 0.3352\n",
      "Epoch 31/100\n",
      "118/118 [==============================] - 1s 5ms/step - loss: 1.0851 - profit_accuracy_121: 0.3717 - val_loss: 1.0930 - val_profit_accuracy_121: 0.3639\n",
      "Epoch 32/100\n",
      "118/118 [==============================] - 1s 5ms/step - loss: 1.0846 - profit_accuracy_121: 0.3776 - val_loss: 1.0880 - val_profit_accuracy_121: 0.4024\n",
      "Epoch 33/100\n",
      "118/118 [==============================] - 1s 4ms/step - loss: 1.0850 - profit_accuracy_121: 0.3844 - val_loss: 1.0883 - val_profit_accuracy_121: 0.3649\n",
      "Epoch 34/100\n",
      "118/118 [==============================] - 1s 5ms/step - loss: 1.0846 - profit_accuracy_121: 0.3786 - val_loss: 1.0895 - val_profit_accuracy_121: 0.3846\n",
      "Epoch 35/100\n",
      "118/118 [==============================] - 1s 5ms/step - loss: 1.0848 - profit_accuracy_121: 0.3862 - val_loss: 1.0901 - val_profit_accuracy_121: 0.3717\n",
      "Epoch 36/100\n",
      "118/118 [==============================] - 1s 5ms/step - loss: 1.0834 - profit_accuracy_121: 0.3878 - val_loss: 1.0869 - val_profit_accuracy_121: 0.3636\n",
      "Epoch 37/100\n",
      "118/118 [==============================] - 1s 5ms/step - loss: 1.0839 - profit_accuracy_121: 0.3936 - val_loss: 1.0877 - val_profit_accuracy_121: 0.3718\n",
      "Epoch 38/100\n",
      "118/118 [==============================] - 1s 5ms/step - loss: 1.0839 - profit_accuracy_121: 0.3762 - val_loss: 1.0877 - val_profit_accuracy_121: 0.4130\n",
      "Epoch 39/100\n",
      "118/118 [==============================] - 1s 4ms/step - loss: 1.0838 - profit_accuracy_121: 0.3899 - val_loss: 1.0888 - val_profit_accuracy_121: 0.3893\n",
      "Epoch 40/100\n",
      "118/118 [==============================] - 1s 5ms/step - loss: 1.0834 - profit_accuracy_121: 0.3957 - val_loss: 1.0913 - val_profit_accuracy_121: 0.3630\n",
      "Epoch 41/100\n",
      "118/118 [==============================] - 1s 6ms/step - loss: 1.0837 - profit_accuracy_121: 0.3873 - val_loss: 1.0923 - val_profit_accuracy_121: 0.3345\n",
      "Epoch 42/100\n",
      "118/118 [==============================] - 1s 6ms/step - loss: 1.0827 - profit_accuracy_121: 0.3739 - val_loss: 1.0949 - val_profit_accuracy_121: 0.3324\n",
      "Epoch 43/100\n",
      "118/118 [==============================] - 1s 6ms/step - loss: 1.0828 - profit_accuracy_121: 0.3721 - val_loss: 1.0882 - val_profit_accuracy_121: 0.3396\n",
      "Epoch 44/100\n",
      "118/118 [==============================] - 1s 5ms/step - loss: 1.0830 - profit_accuracy_121: 0.3905 - val_loss: 1.0992 - val_profit_accuracy_121: 0.3091\n",
      "Epoch 45/100\n",
      "118/118 [==============================] - 1s 5ms/step - loss: 1.0831 - profit_accuracy_121: 0.3871 - val_loss: 1.0877 - val_profit_accuracy_121: 0.3515\n",
      "Epoch 46/100\n",
      "118/118 [==============================] - 1s 6ms/step - loss: 1.0813 - profit_accuracy_121: 0.3865 - val_loss: 1.0908 - val_profit_accuracy_121: 0.3391\n",
      "Epoch 47/100\n",
      "118/118 [==============================] - 0s 4ms/step - loss: 1.0822 - profit_accuracy_121: 0.3930 - val_loss: 1.0873 - val_profit_accuracy_121: 0.3534\n",
      "Epoch 48/100\n",
      "118/118 [==============================] - 1s 5ms/step - loss: 1.0828 - profit_accuracy_121: 0.3916 - val_loss: 1.0905 - val_profit_accuracy_121: 0.3667\n",
      "Epoch 49/100\n",
      "118/118 [==============================] - 1s 4ms/step - loss: 1.0814 - profit_accuracy_121: 0.3848 - val_loss: 1.0865 - val_profit_accuracy_121: 0.4103\n",
      "Epoch 50/100\n",
      "118/118 [==============================] - 1s 5ms/step - loss: 1.0814 - profit_accuracy_121: 0.3855 - val_loss: 1.0897 - val_profit_accuracy_121: 0.3711\n",
      "Epoch 51/100\n",
      "118/118 [==============================] - 1s 5ms/step - loss: 1.0820 - profit_accuracy_121: 0.3858 - val_loss: 1.0895 - val_profit_accuracy_121: 0.3774\n",
      "Epoch 52/100\n",
      "118/118 [==============================] - 0s 4ms/step - loss: 1.0808 - profit_accuracy_121: 0.3822 - val_loss: 1.0896 - val_profit_accuracy_121: 0.3810\n",
      "Epoch 53/100\n",
      "118/118 [==============================] - 1s 5ms/step - loss: 1.0834 - profit_accuracy_121: 0.3864 - val_loss: 1.0881 - val_profit_accuracy_121: 0.3415\n",
      "Epoch 54/100\n",
      "118/118 [==============================] - 1s 6ms/step - loss: 1.0816 - profit_accuracy_121: 0.3821 - val_loss: 1.0874 - val_profit_accuracy_121: 0.3660\n",
      "Epoch 55/100\n",
      "118/118 [==============================] - 1s 6ms/step - loss: 1.0820 - profit_accuracy_121: 0.3881 - val_loss: 1.0911 - val_profit_accuracy_121: 0.3602\n",
      "Epoch 56/100\n",
      "118/118 [==============================] - 1s 5ms/step - loss: 1.0809 - profit_accuracy_121: 0.3923 - val_loss: 1.0886 - val_profit_accuracy_121: 0.3575\n",
      "Epoch 57/100\n",
      "118/118 [==============================] - 1s 5ms/step - loss: 1.0805 - profit_accuracy_121: 0.3899 - val_loss: 1.0868 - val_profit_accuracy_121: 0.4143\n",
      "Epoch 58/100\n",
      "118/118 [==============================] - 1s 5ms/step - loss: 1.0806 - profit_accuracy_121: 0.3835 - val_loss: 1.0891 - val_profit_accuracy_121: 0.3762\n",
      "Epoch 59/100\n",
      "118/118 [==============================] - 1s 6ms/step - loss: 1.0806 - profit_accuracy_121: 0.3827 - val_loss: 1.0889 - val_profit_accuracy_121: 0.3333\n",
      "Epoch 60/100\n",
      "118/118 [==============================] - 0s 4ms/step - loss: 1.0807 - profit_accuracy_121: 0.3880 - val_loss: 1.0898 - val_profit_accuracy_121: 0.3668\n",
      "Epoch 61/100\n",
      "118/118 [==============================] - 1s 4ms/step - loss: 1.0812 - profit_accuracy_121: 0.3872 - val_loss: 1.0899 - val_profit_accuracy_121: 0.3522\n",
      "Epoch 62/100\n",
      "118/118 [==============================] - 0s 4ms/step - loss: 1.0801 - profit_accuracy_121: 0.3857 - val_loss: 1.0872 - val_profit_accuracy_121: 0.3558\n",
      "Epoch 63/100\n",
      "118/118 [==============================] - 0s 4ms/step - loss: 1.0803 - profit_accuracy_121: 0.3958 - val_loss: 1.0900 - val_profit_accuracy_121: 0.3750\n",
      "Epoch 64/100\n",
      "118/118 [==============================] - 1s 5ms/step - loss: 1.0793 - profit_accuracy_121: 0.3777 - val_loss: 1.0864 - val_profit_accuracy_121: 0.3645\n",
      "Epoch 65/100\n",
      "118/118 [==============================] - 0s 4ms/step - loss: 1.0805 - profit_accuracy_121: 0.3968 - val_loss: 1.0903 - val_profit_accuracy_121: 0.3663\n",
      "Epoch 66/100\n",
      "118/118 [==============================] - 1s 5ms/step - loss: 1.0802 - profit_accuracy_121: 0.3851 - val_loss: 1.0902 - val_profit_accuracy_121: 0.3755\n",
      "Epoch 67/100\n",
      "118/118 [==============================] - 1s 5ms/step - loss: 1.0799 - profit_accuracy_121: 0.3807 - val_loss: 1.0890 - val_profit_accuracy_121: 0.3709\n",
      "Epoch 68/100\n",
      "118/118 [==============================] - 1s 5ms/step - loss: 1.0806 - profit_accuracy_121: 0.3887 - val_loss: 1.0920 - val_profit_accuracy_121: 0.3483\n",
      "Epoch 69/100\n",
      "118/118 [==============================] - 1s 4ms/step - loss: 1.0793 - profit_accuracy_121: 0.3897 - val_loss: 1.0940 - val_profit_accuracy_121: 0.3314\n",
      "Epoch 70/100\n",
      "118/118 [==============================] - 1s 5ms/step - loss: 1.0798 - profit_accuracy_121: 0.3927 - val_loss: 1.0959 - val_profit_accuracy_121: 0.3325\n",
      "Epoch 71/100\n",
      "118/118 [==============================] - 1s 5ms/step - loss: 1.0796 - profit_accuracy_121: 0.3848 - val_loss: 1.0914 - val_profit_accuracy_121: 0.3727\n",
      "Epoch 72/100\n",
      "118/118 [==============================] - 1s 5ms/step - loss: 1.0794 - profit_accuracy_121: 0.3807 - val_loss: 1.0873 - val_profit_accuracy_121: 0.3636\n",
      "Epoch 73/100\n",
      "118/118 [==============================] - 1s 5ms/step - loss: 1.0785 - profit_accuracy_121: 0.3978 - val_loss: 1.0891 - val_profit_accuracy_121: 0.3578\n",
      "Epoch 74/100\n",
      "118/118 [==============================] - 1s 5ms/step - loss: 1.0798 - profit_accuracy_121: 0.3835 - val_loss: 1.0871 - val_profit_accuracy_121: 0.3929\n",
      "Epoch 75/100\n",
      "118/118 [==============================] - 0s 4ms/step - loss: 1.0781 - profit_accuracy_121: 0.3871 - val_loss: 1.0917 - val_profit_accuracy_121: 0.3571\n",
      "Epoch 76/100\n",
      "118/118 [==============================] - 0s 4ms/step - loss: 1.0784 - profit_accuracy_121: 0.3900 - val_loss: 1.0866 - val_profit_accuracy_121: 0.3456\n",
      "Epoch 77/100\n",
      "118/118 [==============================] - 1s 5ms/step - loss: 1.0800 - profit_accuracy_121: 0.3783 - val_loss: 1.0918 - val_profit_accuracy_121: 0.3367\n",
      "Epoch 78/100\n",
      "118/118 [==============================] - 1s 5ms/step - loss: 1.0791 - profit_accuracy_121: 0.3922 - val_loss: 1.0878 - val_profit_accuracy_121: 0.3628\n",
      "Epoch 79/100\n",
      "118/118 [==============================] - 1s 5ms/step - loss: 1.0801 - profit_accuracy_121: 0.3974 - val_loss: 1.0855 - val_profit_accuracy_121: 0.3853\n",
      "Epoch 80/100\n",
      "118/118 [==============================] - 1s 4ms/step - loss: 1.0788 - profit_accuracy_121: 0.3846 - val_loss: 1.0889 - val_profit_accuracy_121: 0.3558\n",
      "Epoch 81/100\n",
      "118/118 [==============================] - 1s 5ms/step - loss: 1.0803 - profit_accuracy_121: 0.3897 - val_loss: 1.0863 - val_profit_accuracy_121: 0.3551\n",
      "Epoch 82/100\n",
      "118/118 [==============================] - 1s 5ms/step - loss: 1.0803 - profit_accuracy_121: 0.3913 - val_loss: 1.0886 - val_profit_accuracy_121: 0.3545\n",
      "Epoch 83/100\n",
      "118/118 [==============================] - 1s 5ms/step - loss: 1.0783 - profit_accuracy_121: 0.3791 - val_loss: 1.0872 - val_profit_accuracy_121: 0.3491\n",
      "Epoch 84/100\n",
      "118/118 [==============================] - 1s 4ms/step - loss: 1.0778 - profit_accuracy_121: 0.3929 - val_loss: 1.0862 - val_profit_accuracy_121: 0.3889\n",
      "Epoch 85/100\n",
      "118/118 [==============================] - 0s 4ms/step - loss: 1.0776 - profit_accuracy_121: 0.3893 - val_loss: 1.0905 - val_profit_accuracy_121: 0.3536\n",
      "Epoch 86/100\n",
      "118/118 [==============================] - 0s 4ms/step - loss: 1.0780 - profit_accuracy_121: 0.3901 - val_loss: 1.0870 - val_profit_accuracy_121: 0.4000\n",
      "Epoch 87/100\n",
      "118/118 [==============================] - 1s 5ms/step - loss: 1.0782 - profit_accuracy_121: 0.3942 - val_loss: 1.0855 - val_profit_accuracy_121: 0.3839\n",
      "Epoch 88/100\n",
      "118/118 [==============================] - 1s 5ms/step - loss: 1.0781 - profit_accuracy_121: 0.3895 - val_loss: 1.0864 - val_profit_accuracy_121: 0.3562\n",
      "Epoch 89/100\n",
      "118/118 [==============================] - 1s 5ms/step - loss: 1.0795 - profit_accuracy_121: 0.3832 - val_loss: 1.0881 - val_profit_accuracy_121: 0.3672\n",
      "Epoch 90/100\n",
      "118/118 [==============================] - 0s 4ms/step - loss: 1.0774 - profit_accuracy_121: 0.3885 - val_loss: 1.0865 - val_profit_accuracy_121: 0.3571\n",
      "Epoch 91/100\n",
      "118/118 [==============================] - 1s 5ms/step - loss: 1.0780 - profit_accuracy_121: 0.3939 - val_loss: 1.0864 - val_profit_accuracy_121: 0.3588\n",
      "Epoch 92/100\n",
      "118/118 [==============================] - 1s 4ms/step - loss: 1.0785 - profit_accuracy_121: 0.3866 - val_loss: 1.0884 - val_profit_accuracy_121: 0.3365\n",
      "Epoch 93/100\n",
      "118/118 [==============================] - 1s 4ms/step - loss: 1.0772 - profit_accuracy_121: 0.3982 - val_loss: 1.0860 - val_profit_accuracy_121: 0.4141\n",
      "Epoch 94/100\n",
      "118/118 [==============================] - 1s 6ms/step - loss: 1.0793 - profit_accuracy_121: 0.3981 - val_loss: 1.0912 - val_profit_accuracy_121: 0.3497\n",
      "Epoch 95/100\n",
      "118/118 [==============================] - 1s 6ms/step - loss: 1.0772 - profit_accuracy_121: 0.3959 - val_loss: 1.0883 - val_profit_accuracy_121: 0.3465\n",
      "Epoch 96/100\n",
      "118/118 [==============================] - 1s 6ms/step - loss: 1.0780 - profit_accuracy_121: 0.3793 - val_loss: 1.0891 - val_profit_accuracy_121: 0.3470\n",
      "Epoch 97/100\n",
      "118/118 [==============================] - 1s 6ms/step - loss: 1.0781 - profit_accuracy_121: 0.3869 - val_loss: 1.0871 - val_profit_accuracy_121: 0.3893\n",
      "Epoch 98/100\n",
      "118/118 [==============================] - 1s 9ms/step - loss: 1.0790 - profit_accuracy_121: 0.3910 - val_loss: 1.0938 - val_profit_accuracy_121: 0.3466\n",
      "Epoch 99/100\n",
      "118/118 [==============================] - 1s 7ms/step - loss: 1.0783 - profit_accuracy_121: 0.3953 - val_loss: 1.0869 - val_profit_accuracy_121: 0.3774\n",
      "Epoch 100/100\n",
      "118/118 [==============================] - 1s 6ms/step - loss: 1.0768 - profit_accuracy_121: 0.3895 - val_loss: 1.0873 - val_profit_accuracy_121: 0.3750\n",
      "30/30 [==============================] - 1s 3ms/step\n",
      "Profit_accuracy = 0.375\n",
      "Combine profit_accuracy = 0.4166666567325592\n"
     ]
    }
   ],
   "source": [
    "combined_pred_probs, profit_accuracy = train_lstm_model_combine(X_train_normalized_eco, X_test_normalized_eco, X_train_normalized_tech, X_test_normalized_tech, y_train, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
