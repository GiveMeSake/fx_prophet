{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been successfully written to ./getdata/ig/CS.D.EURUSD.MINI.IP_DAY_2022-11-11_2024-03-28.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import configparser\n",
    "import pytz\n",
    "\n",
    "isLiveApi = 0\n",
    "epic = 'CS.D.EURUSD.MINI.IP'\n",
    "resolution = 'DAY'\n",
    "delta = timedelta(days=20) # historical data can fetch one API call\n",
    "# Ideal: '2006-01-01T00:00:00' -> '2024-03-29T00:00:00'\n",
    "start_date_utc = datetime.strptime('2022-11-11T00:00:00', '%Y-%m-%dT%H:%M:%S')\n",
    "end_date_utc = datetime.strptime('2024-03-29T00:00:00', '%Y-%m-%dT%H:%M:%S')\n",
    "\n",
    "# Define the UTC timezone\n",
    "utc = pytz.utc\n",
    "\n",
    "# Localize the input times to UTC\n",
    "start_date_utc = utc.localize(start_date_utc)\n",
    "end_date_utc = utc.localize(end_date_utc)\n",
    "\n",
    "# Define the target timezone (UTC+7)\n",
    "target_timezone = pytz.timezone('Asia/Bangkok')  # Example timezone for UTC+7\n",
    "\n",
    "# Convert the UTC times to the target timezone\n",
    "start_date_utc_7 = start_date_utc.astimezone(target_timezone)\n",
    "end_date_utc_7 = end_date_utc.astimezone(target_timezone)\n",
    "\n",
    "# Format the dates back to strings if needed\n",
    "start_date_utc_7_str = start_date_utc_7.strftime('%Y-%m-%dT%H:%M:%S')\n",
    "end_date_utc_7_str = end_date_utc_7.strftime('%Y-%m-%dT%H:%M:%S')\n",
    "\n",
    "start_date = datetime.strptime(start_date_utc_7_str, '%Y-%m-%dT%H:%M:%S')\n",
    "end_date = datetime.strptime(end_date_utc_7_str, '%Y-%m-%dT%H:%M:%S')\n",
    "# List to hold all data frames\n",
    "data_frames = []\n",
    "\n",
    "# Read credentials from the configuration file\n",
    "config = configparser.ConfigParser()\n",
    "config.read('ig.cfg')\n",
    "\n",
    "# Function to generate date ranges in 20-day increments\n",
    "def date_range(start_date, end_date, delta):\n",
    "    current_date = start_date\n",
    "    while current_date < end_date:\n",
    "        yield current_date, min(current_date + delta, end_date)\n",
    "        current_date += delta\n",
    "\n",
    "\n",
    "def rename_csv_file_based_on_last_snapshot_time(csv_file_name):\n",
    "    # Step 1: Read the last line of the CSV file\n",
    "    with open(csv_file_name, 'r') as file:\n",
    "        last_line = file.readlines()[-1]\n",
    "    \n",
    "    # Step 2: Extract the snapshotTimeUTC value from the last line\n",
    "    last_line_values = last_line.split(',')\n",
    "    snapshot_time_utc = last_line_values[1].strip()\n",
    "    \n",
    "    # Extract the date part from the snapshotTimeUTC (first 10 characters)\n",
    "    date_part = snapshot_time_utc[:10]\n",
    "    \n",
    "    # Step 3: Construct the new file name\n",
    "    base_name = os.path.basename(csv_file_name)\n",
    "    dir_name = os.path.dirname(csv_file_name)\n",
    "    \n",
    "    # Extract the start part of the file name (up to the last underscore before the date)\n",
    "    name_parts = base_name.rsplit('_', 1)\n",
    "    new_base_name = f\"{name_parts[0]}_{date_part}.csv\"\n",
    "    new_csv_file_name = os.path.join(dir_name, new_base_name)\n",
    "    \n",
    "    # Step 4: Rename the file\n",
    "    os.rename(csv_file_name, new_csv_file_name)\n",
    "    \n",
    "    return new_csv_file_name\n",
    "    \n",
    "if isLiveApi == 0:\n",
    "    API_KEY = config.get('demo', 'api_key')\n",
    "    USERNAME = config.get('demo', 'username')\n",
    "    PASSWORD = config.get('demo', 'password')\n",
    "    BASE_URL = config.get('demo', 'base_url')\n",
    "    ACC_ID = config.get('demo', 'acc_number')\n",
    "else:\n",
    "    API_KEY = config.get('live', 'api_key')\n",
    "    USERNAME = config.get('live', 'username')\n",
    "    PASSWORD = config.get('live', 'password')\n",
    "    BASE_URL = config.get('live', 'base_url')\n",
    "    ACC_ID = config.get('live', 'acc_number')\n",
    "\n",
    "# IG API URLs for demo account\n",
    "LOGIN_URL = BASE_URL + 'session'\n",
    "MARKET_URL = BASE_URL + 'markets'\n",
    "HISTORICAL_DATA_URL = BASE_URL + 'prices/' + epic\n",
    "# HISTORICAL_DATA_URL = BASE_URL + 'prices/{epic}/{resolution}/{start_date}/{end_date}'\n",
    "\n",
    "# Authenticate and obtain access token\n",
    "auth_response = requests.post(LOGIN_URL, json={\n",
    "    'identifier': USERNAME,\n",
    "    'password': PASSWORD\n",
    "}, headers={\n",
    "    'Content-Type': 'application/json; charset=UTF-8',\n",
    "    'Accept': 'application/json; charset=UTF-8',\n",
    "    'X-IG-API-KEY': API_KEY,\n",
    "    'Version': '3',\n",
    "    'IG-ACCOUNT-ID': ACC_ID\n",
    "    }\n",
    ")\n",
    "\n",
    "# Extract the access token from the response\n",
    "access_token = auth_response.json()['oauthToken']['access_token']\n",
    "token_type = auth_response.json()['oauthToken']['token_type']\n",
    "\n",
    "# Loop through each date range and fetch data\n",
    "for start, end in date_range(start_date, end_date, delta):\n",
    "    start_str = start.strftime('%Y-%m-%dT%H:%M:%S')\n",
    "    end_str = end.strftime('%Y-%m-%dT%H:%M:%S')\n",
    "\n",
    "    # Request historical price data for the specified instrument\n",
    "    history_response = requests.get(HISTORICAL_DATA_URL,\n",
    "        headers={\n",
    "            'Content-Type': 'application/json; charset=UTF-8',\n",
    "            'Accept': 'application/json; charset=UTF-8',\n",
    "            'Authorization': f\"{token_type} {access_token}\",\n",
    "            'Version': '3',\n",
    "            'X-IG-API-KEY': API_KEY,\n",
    "            'IG-ACCOUNT-ID': ACC_ID\n",
    "        },\n",
    "        params={\n",
    "            'resolution': resolution,\n",
    "            'from': start_str,\n",
    "            'to': end_str,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Check for successful response\n",
    "    if history_response.status_code == 200:\n",
    "        # Convert JSON response to pandas DataFrame and append to list\n",
    "        data = history_response.json()\n",
    "        df = pd.DataFrame(data['prices'])\n",
    "        data_frames.append(df)\n",
    "    else:\n",
    "        print(f\"Failed to fetch data from {start_str} to {end_str}. Status code: {history_response.status_code}\")\n",
    "        # end = get_date_days_before(end_str, delta)\n",
    "        break\n",
    "\n",
    "# Concatenate all data frames into one\n",
    "if data_frames:\n",
    "    all_data = pd.concat(data_frames, ignore_index=True)\n",
    "    # Write the concatenated data frame to a CSV file\n",
    "    csv_file_name = f'./getdata/ig/{epic}_{resolution}_{start_date.strftime(\"%Y-%m-%d\")}_{end}.csv'\n",
    "    all_data.to_csv(csv_file_name, index=False)\n",
    "    new_csv_file_name = rename_csv_file_based_on_last_snapshot_time(csv_file_name)\n",
    "    print(f\"Data has been successfully written to {new_csv_file_name}\")\n",
    "else:\n",
    "    print(\"No data was fetched.\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformed data saved to ./getdata/ig/CS.D.EURUSD.MINI.IP_DAY_2022-11-11_2024-03-28.csv_format.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "\n",
    "# Function to calculate the average of bid and ask prices\n",
    "def calculate_avg_price(price_str):\n",
    "    price_dict = ast.literal_eval(price_str)\n",
    "    return round(((price_dict['bid'] + price_dict['ask']) / 20000), 7)\n",
    "\n",
    "# Read the original CSV file\n",
    "input_file = new_csv_file_name\n",
    "df = pd.read_csv(input_file)\n",
    "# Calculate the average prices\n",
    "df['time'] = pd.to_datetime(df['snapshotTimeUTC'])\n",
    "df['time'] = df['time'].dt.strftime(\"%Y-%m-%d 00:00:00\")\n",
    "df['o'] = df['openPrice'].apply(calculate_avg_price)\n",
    "df['h'] = df['highPrice'].apply(calculate_avg_price)\n",
    "df['l'] = df['lowPrice'].apply(calculate_avg_price)\n",
    "df['c'] = df['closePrice'].apply(calculate_avg_price)\n",
    "df['complete'] = True\n",
    "\n",
    "# Rename and reorder the columns\n",
    "df.rename(columns={\n",
    "    'snapshotTime': 'datetime',\n",
    "    # 'snapshotTimeUTC': 'time',\n",
    "    'lastTradedVolume': 'volume'\n",
    "}, inplace=True)\n",
    "\n",
    "# Select only the required columns\n",
    "# df = df[['datetime', 'datetimeUTC', 'o', 'h', 'l', 'c', 'volume']]\n",
    "df = df[['time', 'o', 'h', 'l', 'c', 'volume', 'complete']]\n",
    "\n",
    "# Save the transformed DataFrame to a new CSV file\n",
    "# output_file = './getdata/ig/CS.D.EURUSD.MINI.IP_DAY_2006-01-01_2024-05-15_format.csv'\n",
    "output_file = f\"{input_file}_format.csv\"\n",
    "df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Transformed data saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "def merge_csv_files(start_merge, end_merge, directory):\n",
    "    # Convert start and end merge dates to datetime objects\n",
    "    start_date = datetime.strptime(start_merge, '%Y-%m-%d')\n",
    "    end_date = datetime.strptime(end_merge, '%Y-%m-%d')\n",
    "\n",
    "    # List all CSV files in the directory\n",
    "    all_files = [f for f in os.listdir(directory) if f.endswith('_format.csv')]\n",
    "\n",
    "    # Filter files based on the date range in the filename\n",
    "    filtered_files = []\n",
    "    for file in all_files:\n",
    "        # Extract dates from the filename\n",
    "        parts = file.split('_')\n",
    "        file_start_date_str = parts[4]\n",
    "        file_end_date_str = parts[5].replace('_format.csv', '')\n",
    "\n",
    "        try:\n",
    "            file_start_date = datetime.strptime(file_start_date_str, '%Y-%m-%d')\n",
    "            file_end_date = datetime.strptime(file_end_date_str, '%Y-%m-%d')\n",
    "        except ValueError:\n",
    "            print(f\"Filename {file} does not match expected date format and will be skipped.\")\n",
    "            continue\n",
    "\n",
    "        # Check if the file's date range intersects with the specified range\n",
    "        if not (file_end_date < start_date or file_start_date > end_date):\n",
    "            filtered_files.append(file)\n",
    "\n",
    "    # Sort files by their start date\n",
    "    filtered_files.sort(key=lambda x: datetime.strptime(x.split('_')[4], '%Y-%m-%d'))\n",
    "\n",
    "    # Read and concatenate the filtered files\n",
    "    merged_data = pd.concat([pd.read_csv(os.path.join(directory, f)) for f in filtered_files])\n",
    "\n",
    "    # Create the output filename\n",
    "    output_filename = f'CS.D.EURUSD.MINI.IP_DAY_{start_merge}_{end_merge}_format_final.csv'\n",
    "    output_path = os.path.join(directory, output_filename)\n",
    "\n",
    "    # Save the merged data to the output file\n",
    "    merged_data.to_csv(output_path, index=False)\n",
    "    print(f'Merged file saved as {output_path}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'file_start_date' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[82], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Ideal: '2006-01-01' -> '2024-03-29'\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mmerge_csv_files\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m2006-01-01\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m2024-03-29\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./getdata/ig\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[81], line 19\u001b[0m, in \u001b[0;36mmerge_csv_files\u001b[0;34m(start_merge, end_merge, directory)\u001b[0m\n\u001b[1;32m     17\u001b[0m parts \u001b[38;5;241m=\u001b[39m file\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     18\u001b[0m file_start_date_str \u001b[38;5;241m=\u001b[39m parts[\u001b[38;5;241m4\u001b[39m]\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mfile_start_date\u001b[49m)\n\u001b[1;32m     20\u001b[0m file_end_date_str \u001b[38;5;241m=\u001b[39m parts[\u001b[38;5;241m5\u001b[39m]\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_format.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(file_end_date_str)\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'file_start_date' referenced before assignment"
     ]
    }
   ],
   "source": [
    "# Ideal: '2006-01-01' -> '2024-03-29'\n",
    "merge_csv_files('2006-01-01', '2024-03-29', './getdata/ig')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
